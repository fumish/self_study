{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression with laplace prior\n",
    " + In general, laplace prior gives sparse result for regression\n",
    "     + However, it is difficult to deal with it well due to non-differential point at the origin.\n",
    "         + $\\log p(w) \\equiv -1/\\beta \\sum_j |w_j| $, $|w_j|$ is non-differential at the origin.\n",
    " + By the way, non-differential point is eliminated by integrating $|w_j|$:\n",
    "     + $E[|w_j|]$ does not have non-diffenrential point when the distribution is normal distribution.\n",
    "     + It is achieved when we consider about the objective function of variational Bayes.\n",
    "         + $\\mathcal{F} := E[\\log \\frac{q(w)}{p(Y|X,w}p(w)]$\n",
    "         + Here, $\\mathcal{F}$ has a parameter that decides the form of $q(w) = N(w|m, \\Sigma)$, $(m, \\Sigma)$ is the parameter and optimized by it.\n",
    " + In this notebook, the approximated posterior distribution by Variational Bayes is studied.\n",
    "     + The objective function is optimized by a gradient descent method.\n",
    "         + Specifically, the Natural gradient descent is efficient method when we consider about a constrained parameter like positive definite matrix, positive real value, simplex, and so on.\n",
    "         + Thus, we used the natural gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Formulation\n",
    "+ Learning Model:\n",
    "    + $p(y|x,w) = N(y|x \\cdot w, 1), y \\in mathbb{R}, x,w \\in \\mathbb{R}^M$\n",
    "    + $p(w) \\equiv \\exp(-\\frac{1}{\\beta} \\sum_j |w_j|)$, $\\beta$ is hyperparameter.\n",
    "+ Approximated Variational Posterior distribution:\n",
    "    + $q(w) = N(w|m, \\Sigma)$\n",
    "        + $m \\in \\mathbb{R}^M, \\Sigma \\in \\mathbb{R}^{M \\times M}$ is the parameters to be optimized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In this notebook\n",
    "+ We compare the following average generalization error:\n",
    "$$\n",
    "    G(n) = \\frac{1}{L} \\sum_{j=1}^L \\| y - X \\hat{w}(x^l, y^l) \\|^2,\n",
    "$$\n",
    "where $\\hat{w}$ is estimated parameter by $(x^l, y^l)$.  \n",
    "We evaluate the error among Lasso, Ridge, and VB laplace(this calculation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminary\n",
    "## Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import invwishart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LassoCV, Lasso, LassoLarsCV\n",
    "from sklearn.linear_model import RidgeCV, Ridge\n",
    "from sklearn.base import BaseEstimator, RegressorMixin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "## data setting\n",
    "n = 100 # train size\n",
    "M = 400 # # of features\n",
    "n_zero_ind = M // 4 * 3 # # of zero elements in the parameter\n",
    "prob_seed = 20201110 # random seed\n",
    "\n",
    "N = 10000 # test size\n",
    "\n",
    "datasets = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(prob_seed)\n",
    "true_w = np.random.normal(scale = 3, size = M)\n",
    "zero_ind = np.random.choice(M, size = n_zero_ind)\n",
    "true_w[zero_ind] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "ln_vb_params = {\n",
    "    \"pri_beta\": 10,\n",
    "    \"pri_opt_flag\": True,\n",
    "    \"iteration\": 10000,\n",
    "    \"step\": 0.2,\n",
    "    \"is_trace\": False,\n",
    "    \"trace_step\": 100\n",
    "}\n",
    "ln_lasso_params = {\n",
    "    \"fit_intercept\": False,\n",
    "    \"cv\": 5,\n",
    "    \"max_iter\": 10000\n",
    "}\n",
    "ln_ridge_params = {\n",
    "    \"fit_intercept\": False,\n",
    "    \"cv\": 5\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VBLaplace(BaseEstimator, RegressorMixin):\n",
    "    def __init__(\n",
    "        self, pri_beta: float = 20, pri_opt_flag: bool = True,\n",
    "        seed: int = -1, iteration: int = 1000, tol: float = 1e-8, step: float = 0.1,\n",
    "        is_trace: bool = False, trace_step: int = 20\n",
    "    ):\n",
    "        self.pri_beta = pri_beta\n",
    "        self.pri_opt_flag = pri_opt_flag\n",
    "        self.seed = seed\n",
    "        self.iteration = iteration\n",
    "        self.tol = tol\n",
    "        self.step = step\n",
    "        self.is_trace = is_trace\n",
    "        self.trace_step = trace_step\n",
    "        pass\n",
    "    \n",
    "    def _initialization(self, M: int):\n",
    "        seed = self.seed\n",
    "        \n",
    "        if seed > 0:\n",
    "            np.random.seed(seed)\n",
    "        \n",
    "        mean = np.random.normal(size = M)\n",
    "        sigma = invwishart.rvs(df = M+2, scale = np.eye(M), size = 1)\n",
    "        pri_beta = np.random.gamma(shape = 3, size = 1) if self.pri_opt_flag else self.pri_beta\n",
    "        \n",
    "        self.mean_ = mean\n",
    "        self.sigma_ = sigma\n",
    "        self.pri_beta_ = pri_beta\n",
    "        pass\n",
    "    \n",
    "    def _obj_func(self, X:np.ndarray, y:np.ndarray, pri_beta:float, mean:np.ndarray, sigma:np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Calculate objective function.\n",
    "\n",
    "        + Input:\n",
    "            1. X: input matrix (n, M) matrix\n",
    "            2. y: output vector (n, ) matrix\n",
    "            3. mean: mean parameter of vb posterior\n",
    "            4. sigma: covariance matrix of vb posterior\n",
    "\n",
    "        + Output:\n",
    "            value of the objective function.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        n, M = X.shape\n",
    "\n",
    "        sq_sigma_diag = np.sqrt(np.diag(sigma))\n",
    "        log_2pi = np.log(2*np.pi)\n",
    "\n",
    "        F = 0\n",
    "        # const values\n",
    "        F += -M/2*log_2pi -M/2 + M*log_2pi + n*M/2*log_2pi + M*np.log(2*pri_beta)\n",
    "\n",
    "        F += ((y - X@mean)**2).sum()/2 - np.linalg.slogdet(sigma)[1]/2 + np.trace(X.T @ X @ sigma)/2\n",
    "\n",
    "        # term obtained from laplace prior\n",
    "        F += ((mean + 2*sq_sigma_diag*norm.pdf(-mean/sq_sigma_diag)-2*mean*norm.cdf(-mean/sq_sigma_diag))/pri_beta).sum()\n",
    "\n",
    "        return F\n",
    "    \n",
    "    def fit(self, train_X:np.ndarray, train_Y:np.ndarray):\n",
    "        pri_beta = self.pri_beta\n",
    "        iteration = self.iteration\n",
    "        step = self.step\n",
    "        tol = self.tol\n",
    "        \n",
    "        is_trace = self.is_trace\n",
    "        trace_step = self.trace_step\n",
    "        \n",
    "        M = train_X.shape[1]\n",
    "        \n",
    "        if not hasattr(self, \"mean_\"):\n",
    "            self._initialization(M)\n",
    "        \n",
    "        est_mean = self.mean_\n",
    "        est_sigma = self.sigma_\n",
    "        est_pri_beta = self.pri_beta_\n",
    "        \n",
    "        # transformation to natural parameter\n",
    "        theta1 = np.linalg.solve(est_sigma, est_mean)\n",
    "        theta2 = -np.linalg.inv(est_sigma)/2        \n",
    "        \n",
    "        F = []\n",
    "        for ite in range(iteration):\n",
    "            sq_sigma_diag = np.sqrt(np.diag(est_sigma))\n",
    "\n",
    "            # update mean and sigma by natural gradient\n",
    "            dFdnu1 = theta1 - train_Y @ train_X\n",
    "            dFdnu1 += (1 - 2*est_mean/sq_sigma_diag*norm.pdf(-est_mean/sq_sigma_diag) - 2*norm.cdf(-est_mean/sq_sigma_diag)) / est_pri_beta\n",
    "            dFdnu2 = theta2 + train_X.T @ train_X/2\n",
    "            dFdnu2[np.diag_indices(M)] += 1/sq_sigma_diag*norm.pdf(-est_mean/sq_sigma_diag)/est_pri_beta\n",
    "\n",
    "            theta1 += -step * dFdnu1\n",
    "            theta2 += -step * dFdnu2\n",
    "            est_sigma = -np.linalg.inv(theta2)/2\n",
    "            est_mean = est_sigma @ theta1\n",
    "            \n",
    "            # update pri_beta by extreme value\n",
    "            est_pri_beta = ((est_mean + 2*sq_sigma_diag*norm.pdf(-est_mean/sq_sigma_diag)-2*est_mean*norm.cdf(-est_mean/sq_sigma_diag))).mean() if self.pri_opt_flag else pri_beta\n",
    "            current_F = self._obj_func(train_X, train_Y, est_pri_beta, est_mean, est_sigma)\n",
    "            if is_trace and ite % trace_step == 0:\n",
    "                print(current_F, (dFdnu1**2).sum(), (dFdnu2**2).sum())            \n",
    "            \n",
    "            if ite > 0 and np.abs(current_F - F[ite-1]) < tol:\n",
    "                if is_trace:\n",
    "                    print(current_F, (dFdnu1**2).sum(), (dFdnu2**2).sum())                            \n",
    "                break\n",
    "            else:\n",
    "                F.append(current_F)\n",
    "            pass\n",
    "        \n",
    "        \n",
    "        self.F_ = F\n",
    "        self.mean_ = est_mean\n",
    "        self.sigma_ = est_sigma\n",
    "        self.pri_beta_ = est_pri_beta\n",
    "        \n",
    "        return self\n",
    "        pass\n",
    "    \n",
    "    def predict(self, test_X: np.ndarray):\n",
    "        if not hasattr(self, \"mean_\"):\n",
    "            raise ValueError(\"fit has not finished yet, should fit before predict.\")\n",
    "        return test_X @ self.mean_\n",
    "        pass\n",
    "        \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VBNormal(BaseEstimator, RegressorMixin):\n",
    "    def __init__(\n",
    "        self, pri_beta: float = 20, pri_opt_flag: bool = True,\n",
    "        seed: int = -1, iteration: int = 1000, tol: float = 1e-8, step: float = 0.1,\n",
    "        is_trace: bool = False, trace_step: int = 20\n",
    "    ):\n",
    "        self.pri_beta = pri_beta\n",
    "        self.pri_opt_flag = pri_opt_flag\n",
    "        self.seed = seed\n",
    "        self.iteration = iteration\n",
    "        self.tol = tol\n",
    "        self.step = step\n",
    "        self.is_trace = is_trace\n",
    "        self.trace_step = trace_step\n",
    "        pass\n",
    "    \n",
    "    def _initialization(self, M: int):\n",
    "        seed = self.seed\n",
    "        \n",
    "        if seed > 0:\n",
    "            np.random.seed(seed)\n",
    "        \n",
    "        mean = np.random.normal(size = M)\n",
    "        sigma = invwishart.rvs(df = M+2, scale = np.eye(M), size = 1)\n",
    "        pri_beta = np.random.gamma(shape = 3, size = 1) if self.pri_opt_flag else self.pri_beta\n",
    "        \n",
    "        self.mean_ = mean\n",
    "        self.sigma_ = sigma\n",
    "        self.pri_beta_ = pri_beta\n",
    "        pass\n",
    "    \n",
    "    def _obj_func(self, X:np.ndarray, y:np.ndarray, pri_beta:float, mean:np.ndarray, sigma:np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Calculate objective function.\n",
    "\n",
    "        + Input:\n",
    "            1. X: input matrix (n, M) matrix\n",
    "            2. y: output vector (n, ) matrix\n",
    "            3. mean: mean parameter of vb posterior\n",
    "            4. sigma: covariance matrix of vb posterior\n",
    "\n",
    "        + Output:\n",
    "            value of the objective function.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        n, M = X.shape\n",
    "\n",
    "        log_2pi = np.log(2*np.pi)\n",
    "\n",
    "        F = 0\n",
    "        # const values\n",
    "        F += -M/2*log_2pi -M/2 + M*log_2pi + n*M/2*log_2pi + M*np.log(2*pri_beta)\n",
    "\n",
    "        F += ((y - X@mean)**2).sum()/2 - np.linalg.slogdet(sigma)[1]/2 + np.trace(X.T @ X @ sigma)/2\n",
    "\n",
    "        # term obtained from Normal prior\n",
    "        F += pri_beta/2*(mean@mean + np.trace(sigma)) - M/2*np.log(pri_beta) + M/2*log_2pi\n",
    "        \n",
    "        return F\n",
    "    \n",
    "    def fit(self, train_X:np.ndarray, train_Y:np.ndarray):\n",
    "        pri_beta = self.pri_beta\n",
    "        iteration = self.iteration\n",
    "        step = self.step\n",
    "        tol = self.tol\n",
    "        \n",
    "        is_trace = self.is_trace\n",
    "        trace_step = self.trace_step\n",
    "        \n",
    "        M = train_X.shape[1]\n",
    "        \n",
    "        if not hasattr(self, \"mean_\"):\n",
    "            self._initialization(M)\n",
    "        \n",
    "        est_mean = self.mean_\n",
    "        est_sigma = self.sigma_\n",
    "        est_pri_beta = self.pri_beta_\n",
    "                \n",
    "        F = []\n",
    "        X_cov = train_X.T@train_X\n",
    "        \n",
    "        for ite in range(iteration):\n",
    "            sigma_inv = X_cov + est_pri_beta*np.eye(M)\n",
    "            est_mean = np.linalg.solve(sigma_inv, train_Y@train_X)\n",
    "            est_sigma = np.linalg.inv(sigma_inv)\n",
    "            \n",
    "            # update pri_beta by extreme value\n",
    "            est_pri_beta = M/(est_mean@est_mean + np.trace(est_sigma)) if self.pri_opt_flag else pri_beta\n",
    "            current_F = self._obj_func(train_X, train_Y, est_pri_beta, est_mean, est_sigma)\n",
    "            if is_trace and ite % trace_step == 0:\n",
    "                print(current_F, (dFdnu1**2).sum(), (dFdnu2**2).sum())            \n",
    "            \n",
    "            if ite > 0 and np.abs(current_F - F[ite-1]) < tol:\n",
    "                if is_trace:\n",
    "                    print(current_F, (dFdnu1**2).sum(), (dFdnu2**2).sum())                            \n",
    "                break\n",
    "            else:\n",
    "                F.append(current_F)\n",
    "            pass\n",
    "        \n",
    "        \n",
    "        self.F_ = F\n",
    "        self.mean_ = est_mean\n",
    "        self.sigma_ = est_sigma\n",
    "        self.pri_beta_ = est_pri_beta\n",
    "        \n",
    "        return self\n",
    "        pass\n",
    "    \n",
    "    def predict(self, test_X: np.ndarray):\n",
    "        if not hasattr(self, \"mean_\"):\n",
    "            raise ValueError(\"fit has not finished yet, should fit before predict.\")\n",
    "        return test_X @ self.mean_\n",
    "        pass\n",
    "        \n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment part\n",
    "+ By some datasets are used for train and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1461.9838912138146 1124.870327518092 1104.1059953574977 1124.8668245614854\n",
      "1368.3021266088742 1165.2101176597287 1114.1283002183252 1164.2810702444158\n",
      "1275.4470990418904 1156.847781586961 1134.4315138633813 1156.0547159302596\n",
      "1341.2572072919197 1147.8052513854382 1099.6858612929664 1147.258001717741\n",
      "1324.9654276329397 1109.5695601682235 1111.5140446544758 1108.7753879940678\n",
      "1321.6759920822956 1083.468541201092 1040.570159448133 1081.999100566201\n",
      "1417.3069416999758 1139.8972331920138 1146.3674144477707 1139.6596183102097\n",
      "1552.2007092471802 1120.7322178614534 1068.1965252989603 1120.193703034443\n",
      "1416.5964271053556 1242.2725195715225 1216.6457087138267 1241.92765913438\n",
      "1256.4703819952676 1081.2767336159477 1059.8582486753278 1081.12029440626\n",
      "1355.001674262608 1118.8903210815572 1098.2060133590692 1118.8840141143671\n",
      "1490.0959648103294 1170.3027963395496 1156.2975809997133 1170.3062330415985\n",
      "1487.3920477180225 1239.238264642166 1223.0511433183476 1239.237304256981\n",
      "1275.6351780447121 1154.924534465363 1107.661391163891 1154.915428481425\n",
      "1325.512423466971 1118.4134799149822 1085.181683855377 1117.464725143319\n",
      "1456.7831943948433 1176.2832769616464 1167.0114096956306 1176.2788801011202\n",
      "1391.660938389142 1160.6772671493525 1120.3986246982495 1160.4723566307603\n",
      "1439.0031481986152 1126.6722293966861 1070.6415395064867 1126.6823955654363\n",
      "1537.901867176103 1228.336920840511 1233.331895752618 1228.3510320778014\n",
      "1401.5934486645608 1106.4289537482475 1097.773577335205 1105.4572411267832\n",
      "1652.108718679123 1166.1534989416737 1211.575035454148 1166.164724693622\n",
      "1387.1905777908194 1090.268322978472 1078.9669972679706 1090.2668786612235\n",
      "1504.1268866481605 1223.7602486056087 1235.7020672060362 1224.1166894758894\n",
      "1556.0165122366302 1203.1233237570277 1201.2779363146092 1203.384108620243\n",
      "1312.423380197532 1156.8159097653072 1137.1160006350137 1156.829454941855\n",
      "1462.5562089465805 1150.2180064706913 1172.5893232769872 1149.6948971279296\n",
      "1252.7530876366104 1203.929363855358 1150.3564698614175 1203.9446654329756\n",
      "1394.4792640418534 1163.702798061264 1141.4165078006718 1162.935532634877\n",
      "1312.0828745631773 1159.8001596133283 1122.3561961349242 1159.797992009566\n",
      "1502.0465761625085 1115.346957199474 1072.304628917499 1114.893190281778\n",
      "1388.207142468936 1138.0542218926475 1132.2229785384623 1138.059669451024\n",
      "1366.975429231961 1091.9621673752981 1046.419715131331 1091.4814458340695\n",
      "1452.7747725560603 1165.6423207707965 1178.6592790715151 1165.6368498799966\n",
      "1549.3604192481118 1262.083045946 1286.1584254591091 1261.9795824453875\n",
      "1497.0250472594398 1143.7658104417574 1167.2687693088342 1143.7682584653185\n",
      "1317.56269588309 1092.5224790041227 1059.4461303347393 1092.5269893249153\n",
      "1462.977673598245 1154.1075945723328 1103.5457285227483 1153.4920632834107\n",
      "1319.8959965589313 1126.6406761230735 1109.3004651460515 1126.6416027882633\n",
      "1340.7253844173779 1239.9012336683618 1219.6849890413923 1239.2969042629027\n",
      "1458.3663884191092 1055.8685195874339 1035.8512366726018 1054.7520322474309\n",
      "1428.4872464926977 1140.080775359849 1165.9386861977391 1140.0854602775662\n",
      "1454.3028737436782 1172.9942170277127 1198.9461299986515 1173.0003576499305\n",
      "1411.561065250145 1247.1452489936373 1236.933356490452 1247.0635107043815\n",
      "1491.9385269633128 1172.5667270554075 1180.7221671087943 1172.578578891736\n",
      "1528.8873223554979 1127.39083082316 1145.0630527606427 1127.388318468127\n",
      "1349.4690894629437 1144.0175582439388 1116.0285958178322 1144.0311707211984\n",
      "1382.8009291677752 1179.378614269336 1168.7651593682858 1179.3751457607939\n",
      "1494.874517038449 1109.728305443439 1111.457963029605 1108.2650536314593\n",
      "1427.099032055386 1123.1426360927667 1094.347701868455 1123.1359617323797\n",
      "1487.7789712404308 1200.3239015269282 1199.4904861017928 1200.6880910671396\n",
      "1449.7107789499707 1224.1008992258205 1198.0324029101887 1224.0934143064055\n",
      "1525.0956476644162 1146.5426644016673 1129.4817928500347 1146.4996131486816\n",
      "1418.4125533804088 1134.4527880020516 1094.0045646017782 1134.4431976228504\n",
      "1450.8704984374658 1180.5653259997246 1189.9170220389644 1179.9090224388704\n",
      "1484.5465427272575 1075.534528023487 1052.0460292957152 1075.5639201504168\n",
      "1421.8526221096058 1226.0744873862561 1211.5306845641037 1226.0769171148577\n",
      "1560.086294955678 1203.0026853594504 1137.6765772659642 1202.296991393394\n",
      "1245.1799973274517 1108.7022725755362 1049.715533794571 1108.2713150487639\n",
      "1331.7905848421995 1148.6187058913088 1117.4639704134424 1148.9235763560964\n",
      "1573.048974783761 1181.0225143080143 1159.1772683719587 1179.9682034119617\n",
      "1389.58258723818 1174.5752525505072 1140.5676759645648 1174.5792722386357\n",
      "1444.8223382322174 1096.6523002576187 1080.8660399129565 1096.5014832478364\n",
      "1431.1191517666052 1171.0358559684514 1152.2248519785096 1171.0363706401238\n",
      "1559.019333180014 1247.574258739958 1253.0092447417055 1247.209842511425\n",
      "1427.8307469923918 1165.607549808712 1136.455987830351 1165.6222284184873\n",
      "1336.256188875993 1096.0891809297743 1099.6429131267555 1095.3616231745573\n",
      "1504.0358175721685 1111.37828969873 1076.8044606677695 1110.6585943359512\n",
      "1302.6145352127987 1131.9852016047637 1072.1017602916845 1131.5328739410204\n",
      "1505.3591722457675 1082.0277477445677 1088.0375389585406 1082.0371330104178\n",
      "1254.9819939632919 1115.3881563761747 1062.1859979058584 1114.4605022621997\n",
      "1377.349555700739 1181.7189049458655 1153.9297787483113 1181.0113817299582\n",
      "1266.2056330106032 1120.8357781699922 1056.8511458091145 1120.7592423263281\n",
      "1445.2677705321018 1163.4667094112156 1185.046278833977 1163.4735880801986\n",
      "1321.5576430526562 1121.398987630532 1060.7729969044838 1120.677538280094\n",
      "1527.3484353149693 1209.6533149630259 1230.8776319947963 1209.654821975107\n",
      "1558.4178472288459 1195.2032750893422 1178.0278674195797 1195.204934876209\n",
      "1208.381169405586 1186.789086398178 1136.614767419864 1186.3301507002798\n",
      "1312.3050158905514 1148.7592977232282 1146.546918803357 1148.7612392132214\n",
      "1531.0035398889493 1167.6514085365486 1162.5871439904925 1167.6697393380475\n",
      "1542.001363770625 1111.9356460673864 1116.8365157233768 1111.3630000283322\n",
      "1554.8981201270794 1170.7031666705516 1133.5621312551148 1170.7037360622865\n",
      "1545.8669429040967 1194.6726938420104 1173.8110674468996 1194.2292368001376\n",
      "1375.1717203752673 1218.3244073422388 1195.222556675629 1218.323216871743\n",
      "1477.9098649714053 1150.0869705090886 1143.771782433543 1149.09707665494\n",
      "1456.785375854604 1104.3528773458056 1065.3960311508822 1104.1780704518872\n",
      "1449.934536640768 1115.4490043323112 1109.9592398274476 1115.448562715473\n",
      "1227.9858637488073 1109.4111495428263 1062.614491988372 1109.4080175412503\n",
      "1395.313597074532 1222.4887539411022 1199.3301188034332 1222.3694126916535\n",
      "1358.3824552585443 1202.3390767536773 1156.9295194523615 1202.3490617931702\n",
      "1543.3665145543914 1253.1156793859327 1239.4686666536356 1253.1013393518806\n",
      "1410.8004073172453 1125.7191658334552 1098.9536349119264 1125.718420074096\n",
      "1339.3643207650593 1135.29557202187 1121.218739469882 1134.201544007007\n",
      "1479.773963129007 1202.4229351626157 1198.4004376106905 1202.430510493439\n",
      "1296.1142155277896 1069.2139358077022 1013.5013931273523 1069.2272458356683\n",
      "1356.907787295314 1171.0439117183969 1116.5908422629543 1170.2645798529957\n",
      "1225.7667059539233 1164.143890558875 1135.0276033916603 1163.2996296224908\n",
      "1551.8233378954405 1181.8324713496922 1179.793506014023 1181.5101846578116\n",
      "1529.4820398556042 1178.9197939887247 1176.0692593236902 1178.5341445482518\n",
      "1065.1681982012801 1112.225764959487 1031.199964418893 1112.2172974055968\n",
      "1289.423362026189 1108.8446902020858 1057.8817719831454 1107.9341177275778\n"
     ]
    }
   ],
   "source": [
    "sq_error_vb_laplace = np.zeros(datasets)\n",
    "sq_error_vb_normal = np.zeros(datasets)\n",
    "sq_error_lasso = np.zeros(datasets)\n",
    "sq_error_ridge = np.zeros(datasets)\n",
    "\n",
    "for dataset_ind in range(datasets):\n",
    "    vb_laplace_obj = VBLaplace(**ln_vb_params)\n",
    "    vb_normal_obj = VBNormal(**ln_vb_params)\n",
    "    lasso_obj = LassoCV(**ln_lasso_params)\n",
    "    ridge_obj = RidgeCV(**ln_ridge_params)    \n",
    "    # data generation\n",
    "    train_X = np.random.normal(size = (n, M))\n",
    "    train_Y = train_X @ true_w + np.random.normal(size = n)\n",
    "\n",
    "    lasso_obj.fit(train_X, train_Y)\n",
    "    ridge_obj.fit(train_X, train_Y)\n",
    "    vb_laplace_obj.fit(train_X, train_Y)\n",
    "    vb_normal_obj.fit(train_X, train_Y)\n",
    "\n",
    "    test_X = np.random.normal(size = (N, M))\n",
    "    test_Y = test_X @ true_w + np.random.normal(size = N)\n",
    "    \n",
    "    sq_error_lasso[dataset_ind] = ((test_X @ lasso_obj.coef_- test_Y)**2).mean()\n",
    "    sq_error_ridge[dataset_ind] = ((test_X @ ridge_obj.coef_- test_Y)**2).mean()\n",
    "    sq_error_vb_laplace[dataset_ind] = ((test_X @ vb_laplace_obj.mean_- test_Y)**2).mean()\n",
    "    sq_error_vb_normal[dataset_ind] = ((test_X @ vb_normal_obj.mean_- test_Y)**2).mean()\n",
    "    print(\n",
    "        sq_error_lasso[dataset_ind]\n",
    "        , sq_error_ridge[dataset_ind]\n",
    "        , sq_error_vb_laplace[dataset_ind]\n",
    "        , sq_error_vb_normal[dataset_ind]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1414.7993436308964 1155.1150278682308 1134.7270693182786 1154.8395830368654\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    sq_error_lasso.mean()\n",
    "    , sq_error_ridge.mean()\n",
    "    , sq_error_vb_laplace.mean()\n",
    "    , sq_error_vb_normal.mean()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102.72026157053018"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sq_error_lasso.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  0.        ,  0.        ,  0.        , -0.76668197,\n",
       "        0.07758316,  0.        , -0.0995576 ,  0.        , -0.90644613,\n",
       "        0.        ,  0.        , -1.16645995,  1.00565264, -1.27819984,\n",
       "        0.30094245, -0.627204  ,  3.38047157,  1.22344481,  0.0473449 ,\n",
       "       -0.03814966,  0.        ,  0.        ,  3.52029545,  0.        ,\n",
       "        4.81596502,  0.        , -3.25347577,  0.66002526,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  2.63380714,  1.33677451,\n",
       "        0.        , -2.69622796,  3.58475763,  0.        ,  0.        ,\n",
       "        2.71306942,  0.        ,  1.88162727, -2.59235379,  3.48865255,\n",
       "        0.        ,  0.        ,  0.        ,  2.00792993, -0.4410573 ,\n",
       "        0.        , -4.15620415,  3.9016001 ,  0.        ,  6.5176337 ,\n",
       "       -3.96303101, -0.03691128,  1.35713552,  2.36783035,  0.        ,\n",
       "        0.        ,  0.18762913, -1.26910508, -3.83069906,  0.26426108,\n",
       "       -1.0842188 ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        2.82239462,  0.16265215,  0.        ,  0.        ,  0.        ,\n",
       "        1.42660064, -0.55960312, -1.35619478, -5.20937925,  0.        ,\n",
       "       -0.97513867,  0.        ,  5.61036017, -2.75649377, -4.37142083,\n",
       "        0.6528961 ,  3.65665585,  4.40974135, -0.18317878, -3.07988735,\n",
       "        0.        , -2.53854171,  3.27560309,  0.        ,  7.81317023,\n",
       "        0.        ,  0.        ,  0.        , -4.21542812,  0.        ,\n",
       "       -5.32304818,  0.        ,  0.        ,  1.46339651, -2.80157424,\n",
       "        2.40381311,  1.93661268,  0.        , -3.68530166,  3.68187137,\n",
       "        4.6779508 ,  1.30380009,  0.        ,  0.17820812,  0.        ,\n",
       "       -4.04242208,  0.        ,  0.        ,  0.        , -1.69246901,\n",
       "        1.35555417,  2.7963506 ,  0.        ,  3.07879094, -3.43986175,\n",
       "       -5.22595885,  0.39853712,  3.12602451, -4.18756991,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  4.31956641,\n",
       "       -1.51861904, -4.10136198,  0.30293722,  0.        ,  0.        ,\n",
       "        0.        , -0.21288762,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  2.77619219,  3.10719351,\n",
       "       -2.62922584,  1.34767249,  1.17694762,  0.        ,  0.82839278,\n",
       "        0.        , -2.43049864, -4.70204062,  0.31933329,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        , -3.37294018,\n",
       "        0.        ,  0.49227329,  1.88058534, -2.15501367,  0.        ,\n",
       "        2.51412901, -2.62329196,  1.88505711,  0.        , -1.44365597,\n",
       "        0.        , -5.9506191 ,  4.61010472, -0.3914984 , -2.56273853,\n",
       "        0.        ,  0.        , -0.03003618,  2.00666083, -1.11247555,\n",
       "        5.80470762, -0.75163024, -0.23380517,  0.67443324,  1.1577714 ,\n",
       "       -1.46410966,  0.        , -3.65760862, -1.67716493, -2.42723579,\n",
       "        0.        , -1.72682638, -0.84503449, -0.45390357,  0.        ,\n",
       "        0.        ,  0.        , -3.61775795, -1.11710953,  3.11547965,\n",
       "       -3.40428624,  1.72675508,  0.        , -1.09670842,  2.21402638,\n",
       "        0.        ,  5.68384374, 10.00605115,  0.        ,  0.        ,\n",
       "        0.        ,  0.        , -4.4211814 ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        , -1.12343038,  0.        ,  0.18522035,\n",
       "       -1.45337341,  0.        ,  2.88056761,  0.47835039,  1.22293896,\n",
       "        6.02282615, -0.55582293,  2.80719361, -9.04599777,  1.31709791,\n",
       "        0.        ,  1.41886036, -2.11526084,  1.31447898,  0.        ,\n",
       "       -1.88083653, -1.16022406,  0.        ,  0.        , -3.53824703,\n",
       "       -0.63372719,  0.        , -6.38107692,  5.22173563, -1.72092071,\n",
       "        1.34919695,  3.33451544,  2.68218189,  0.        , -9.42245829,\n",
       "        4.31844858,  0.        , -0.27071409, -1.75983284,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        , -3.71381013,\n",
       "       -4.36134198,  0.        ,  2.86892206,  4.43305466,  0.        ,\n",
       "        0.        ,  3.6775742 ,  0.        ,  0.        ,  0.        ,\n",
       "        1.08341558, -3.30639853,  2.06305632,  0.33057913,  0.19251094,\n",
       "        0.        ,  0.        , -0.88649413,  0.93654874, -0.92907885,\n",
       "        0.        ,  1.98561399,  0.        , -6.21216341, -1.44129124,\n",
       "        0.        , -2.44364738,  0.        ,  0.        , -4.60931771,\n",
       "        0.        , -7.1060601 , -0.25450493,  3.80237406, -1.03990865,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.50845406,\n",
       "        1.43603562,  0.        , -1.20824397,  0.34298881,  0.        ,\n",
       "       -0.10126385,  2.32899934,  0.        ,  0.        ,  2.53418494,\n",
       "        1.09601969, -1.3079937 , -1.43857833,  1.69509419,  0.        ,\n",
       "       -0.32535721,  1.34677404,  0.        ,  1.6209045 ,  0.        ,\n",
       "        0.37218781,  0.10660404, -2.18739269, -0.21103329,  3.88906184,\n",
       "        0.14417489, -0.44632924,  0.        , -3.08581264,  0.        ,\n",
       "       -0.64647768,  0.        ,  0.        ,  0.        ,  3.45132807,\n",
       "       -5.00554243, -0.69960904, -1.49639685,  0.        ,  0.        ,\n",
       "        0.        ,  0.82544241, -1.59146572,  2.98871443,  3.62286903,\n",
       "        1.42762964,  0.        ,  0.        ,  2.47423043,  0.        ,\n",
       "        0.        , -3.93509534, -3.67117534, -1.66381531,  1.56373128,\n",
       "       -2.54003703,  1.59270721,  0.        , -3.34179223,  0.        ,\n",
       "        1.65598468, -0.63192153,  3.2586072 ,  3.79248979,  1.8805576 ,\n",
       "        0.        ,  0.        ,  0.67639555,  2.12813618,  0.        ,\n",
       "       -2.03734815, -0.92737418, -0.76446219,  0.26149847, -1.01734604,\n",
       "        0.        ,  0.        ,  0.        ,  2.90633058, -1.7841167 ,\n",
       "        0.        ,  2.43839142,  1.72382498,  0.91379309, -4.73625657,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  1.76208384,\n",
       "       -5.52170122, -5.29286887,  3.40864884,  1.99232504,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  5.62730881,  0.        ,\n",
       "        0.        , -2.27439354,  0.4710198 ,  0.        ,  2.47498796,\n",
       "        0.        , -0.08208131, -4.96373925, -0.44566027,  0.        ,\n",
       "       -6.48977714,  0.        ,  0.        ,  2.27450145, -3.93815903,\n",
       "        3.3400167 ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.463294  ,  0.        , -0.97093426,\n",
       "        0.        ,  2.09674151,  2.34015509,  0.        ,  2.04765046,\n",
       "        0.        ,  2.79378918, -3.53025833,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        1.34513262,  1.83249431,  0.        ,  2.75072664, -0.09395582,\n",
       "        0.        , -4.52339131,  0.        ,  0.        ,  0.        ,\n",
       "        0.        , -0.13120524, -1.74165902,  0.        ,  2.79691941,\n",
       "        2.63991594,  0.        ,  3.2993635 , -2.01361589,  2.02896249,\n",
       "        1.14511249, -0.30690699,  2.8638261 ,  0.        , -1.90000553,\n",
       "        2.10753598,  1.25927173, -2.0796748 ,  0.        ,  0.8230681 ,\n",
       "        2.19885118,  2.00914115,  8.62446715,  0.        , -1.25440593,\n",
       "        2.36717472,  1.67806024,  0.45576057,  0.        ,  2.94290434,\n",
       "       -6.67368945,  4.84287936,  0.        , -1.54032161,  0.        ,\n",
       "       -1.45007199,  0.        ,  0.        ,  0.14619982,  1.24542242,\n",
       "       -1.1466085 ,  0.        ,  0.        , -0.87419234,  4.46233691,\n",
       "       -1.71233723, -5.01433667,  2.26989479,  3.50090393,  0.        ,\n",
       "        1.14726699,  0.        , -6.01328696,  1.50616649,  0.        ,\n",
       "        1.71264187,  0.        ,  1.61466992,  0.43339527,  0.        ,\n",
       "        0.        ,  0.04176741, -2.46692818, -1.69221917,  0.        ,\n",
       "        4.96165716,  5.70358511,  0.        ,  1.64405708,  3.47260936,\n",
       "       -4.14704621,  1.13520012,  4.72538745,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        , -0.72411832,  2.05994544,\n",
       "       -2.04054909, -5.93196356,  0.        , -6.38319172,  2.66130388,\n",
       "        0.54021669,  2.11793369, -5.40525212,  1.69551788,  0.        ,\n",
       "        0.        ,  1.63502311, -0.87789452, -1.26780838,  0.        ,\n",
       "       -1.69376272,  1.42150321,  0.75101169,  1.70050797,  0.        ,\n",
       "        6.76881923,  0.        , -1.72645458, -0.34222196,  0.        ,\n",
       "        0.        ,  0.        ,  4.20692684, -1.49838649, -0.42849111,\n",
       "        3.27441965,  0.        ,  0.        ,  1.33915726, -2.84127863,\n",
       "        0.        , -1.43255817,  0.        , -3.91065627,  0.        ,\n",
       "        3.28198124,  0.        ,  1.08150973, -0.74044771, -5.10018979,\n",
       "        0.98746237,  0.        ,  2.98754322, -3.93588581, -2.10669405,\n",
       "        0.        ,  0.        , -1.75450388,  0.80332304,  0.        ,\n",
       "        2.25687791,  0.        , -3.02195407, -0.34214784, -1.19599784,\n",
       "        0.        ,  0.        ,  1.20211888,  4.74596595, -3.36959436])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".py",
    "format_name": "light",
    "format_version": "1.4",
    "jupytext_version": "1.1.3"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
