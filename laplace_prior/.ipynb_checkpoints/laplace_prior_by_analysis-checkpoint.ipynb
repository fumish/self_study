{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression with laplace prior\n",
    " + In general, laplace prior gives sparse result for regression\n",
    "     + However, it is difficult to deal with it well due to non-differential point at the origin.\n",
    "         + $\\log p(w) \\equiv -1/\\beta \\sum_j |w_j| $, $|w_j|$ is non-differential at the origin.\n",
    " + By the way, non-differential point is eliminated by integrating $|w_j|$:\n",
    "     + $E[|w_j|]$ does not have non-diffenrential point when the distribution is normal distribution.\n",
    "     + It is achieved when we consider about the objective function of variational Bayes.\n",
    "         + $\\mathcal{F} := E[\\log \\frac{q(w)}{p(Y|X,w}p(w)]$\n",
    "         + Here, $\\mathcal{F}$ has a parameter that decides the form of $q(w) = N(w|m, \\Sigma)$, $(m, \\Sigma)$ is the parameter and optimized by it.\n",
    " + In this notebook, the approximated posterior distribution by Variational Bayes is studied.\n",
    "     + The objective function is optimized by a gradient descent method.\n",
    "         + Specifically, the Natural gradient descent is efficient method when we consider about a constrained parameter like positive definite matrix, positive real value, simplex, and so on.\n",
    "         + Thus, we used the natural gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Formulation\n",
    "+ Learning Model:\n",
    "    + $p(y|x,w) = N(y|x \\cdot w, 1), y \\in mathbb{R}, x,w \\in \\mathbb{R}^M$\n",
    "    + $p(w) \\equiv \\exp(-\\frac{1}{\\beta} \\sum_j |w_j|)$, $\\beta$ is hyperparameter.\n",
    "+ Approximated Variational Posterior distribution:\n",
    "    + $q(w) = N(w|m, \\Sigma)$\n",
    "        + $m \\in \\mathbb{R}^M, \\Sigma \\in \\mathbb{R}^{M \\times M}$ is the parameters to be optimized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In this notebook\n",
    "+ We compare the following average generalization error:\n",
    "$$\n",
    "    G(n) = \\frac{1}{L} \\sum_{j=1}^L \\| y - X \\hat{w}(x^l, y^l) \\|^2,\n",
    "$$\n",
    "where $\\hat{w}$ is estimated parameter by $(x^l, y^l)$.  \n",
    "We evaluate the error among Lasso, Ridge, and VB laplace(this calculation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminary\n",
    "## Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import invwishart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LassoCV, Lasso, LassoLarsCV\n",
    "from sklearn.linear_model import RidgeCV, Ridge\n",
    "from sklearn.linear_model import ARDRegression\n",
    "from sklearn.base import BaseEstimator, RegressorMixin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "## data setting\n",
    "n = 100 # train size\n",
    "M = 150 # # of features\n",
    "n_zero_ind = M//2 # # of zero elements in the parameter\n",
    "prob_seed = 20201110 # random seed\n",
    "\n",
    "N = 10000 # test size\n",
    "\n",
    "datasets = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(prob_seed)\n",
    "true_w = np.random.normal(scale = 3, size = M)\n",
    "zero_ind = np.random.choice(M, size = n_zero_ind)\n",
    "true_w[zero_ind] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ln_vb_params = {\n",
    "    \"pri_beta\": 10,\n",
    "    \"pri_opt_flag\": True,\n",
    "    \"variance\": \"diag\",\n",
    "    \"iteration\": 10000,\n",
    "    \"step\": 0.2,\n",
    "    \"is_trace\": False,\n",
    "    \"trace_step\": 100\n",
    "}\n",
    "ln_lasso_params = {\n",
    "    \"fit_intercept\": False,\n",
    "    \"cv\": 5,\n",
    "    \"max_iter\": 10000\n",
    "}\n",
    "ln_ridge_params = {\n",
    "    \"fit_intercept\": False,\n",
    "    \"cv\": 5\n",
    "}\n",
    "ln_ard_params = {\n",
    "    \"fit_intercept\": False\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VBLaplace(BaseEstimator, RegressorMixin):\n",
    "    def __init__(\n",
    "        self, pri_beta: float = 20, pri_opt_flag: bool = True, variance: str = \"full\",\n",
    "        seed: int = -1, iteration: int = 1000, tol: float = 1e-8, step: float = 0.1,\n",
    "        is_trace: bool = False, trace_step: int = 20\n",
    "    ):\n",
    "        self.pri_beta = pri_beta\n",
    "        self.pri_opt_flag = pri_opt_flag\n",
    "        self.variance = variance\n",
    "        self.seed = seed\n",
    "        self.iteration = iteration\n",
    "        self.tol = tol\n",
    "        self.step = step\n",
    "        self.is_trace = is_trace\n",
    "        self.trace_step = trace_step\n",
    "        pass\n",
    "    \n",
    "    def _initialization(self, M: int):\n",
    "        seed = self.seed\n",
    "        \n",
    "        if seed > 0:\n",
    "            np.random.seed(seed)\n",
    "        \n",
    "        mean = np.random.normal(size = M)\n",
    "        sigma = invwishart.rvs(df = M+2, scale = np.eye(M), size = 1)\n",
    "        if self.variance == \"diag\":\n",
    "            sigma = np.diag(np.diag(sigma))\n",
    "        pri_beta = np.random.gamma(shape = 3, size = 1) if self.pri_opt_flag else self.pri_beta\n",
    "        \n",
    "        self.mean_ = mean\n",
    "        self.sigma_ = sigma\n",
    "        self.pri_beta_ = pri_beta\n",
    "        pass\n",
    "    \n",
    "    def _obj_func(self, X:np.ndarray, y:np.ndarray, pri_beta:float, mean:np.ndarray, sigma:np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Calculate objective function.\n",
    "\n",
    "        + Input:\n",
    "            1. X: input matrix (n, M) matrix\n",
    "            2. y: output vector (n, ) matrix\n",
    "            3. mean: mean parameter of vb posterior\n",
    "            4. sigma: covariance matrix of vb posterior\n",
    "\n",
    "        + Output:\n",
    "            value of the objective function.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        n, M = X.shape\n",
    "\n",
    "        sq_sigma_diag = np.sqrt(np.diag(sigma))\n",
    "        log_2pi = np.log(2*np.pi)\n",
    "\n",
    "        F = 0\n",
    "        # const values\n",
    "        F += -M/2*log_2pi -M/2 + M*log_2pi + n*M/2*log_2pi + M*np.log(2*pri_beta)\n",
    "\n",
    "        F += ((y - X@mean)**2).sum()/2 - np.linalg.slogdet(sigma)[1]/2 + np.trace(X.T @ X @ sigma)/2\n",
    "\n",
    "        # term obtained from laplace prior\n",
    "        F += ((mean + 2*sq_sigma_diag*norm.pdf(-mean/sq_sigma_diag)-2*mean*norm.cdf(-mean/sq_sigma_diag))/pri_beta).sum()\n",
    "\n",
    "        return F\n",
    "    \n",
    "    def fit(self, train_X:np.ndarray, train_Y:np.ndarray):\n",
    "        pri_beta = self.pri_beta\n",
    "        iteration = self.iteration\n",
    "        step = self.step\n",
    "        tol = self.tol\n",
    "        \n",
    "        is_trace = self.is_trace\n",
    "        trace_step = self.trace_step\n",
    "        \n",
    "        M = train_X.shape[1]\n",
    "        \n",
    "        if not hasattr(self, \"mean_\"):\n",
    "            self._initialization(M)\n",
    "        \n",
    "        est_mean = self.mean_\n",
    "        est_sigma = self.sigma_\n",
    "        est_pri_beta = self.pri_beta_\n",
    "        \n",
    "        # transformation to natural parameter\n",
    "        theta1 = np.linalg.solve(est_sigma, est_mean)\n",
    "        theta2 = -np.linalg.inv(est_sigma)/2        \n",
    "        \n",
    "        F = []\n",
    "        \n",
    "        cov_X = train_X.T @ train_X\n",
    "        cov_YX = train_Y @ train_X\n",
    "        for ite in range(iteration):\n",
    "            sq_sigma_diag = np.sqrt(np.diag(est_sigma))\n",
    "\n",
    "            # update mean and sigma by natural gradient\n",
    "            dFdnu1 = theta1 - cov_YX\n",
    "            dFdnu1 += (1 - 2*est_mean/sq_sigma_diag*norm.pdf(-est_mean/sq_sigma_diag) - 2*norm.cdf(-est_mean/sq_sigma_diag)) / est_pri_beta\n",
    "            dFdnu2 = theta2 + cov_X/2\n",
    "            if self.variance == \"diag\":\n",
    "                dFdnu2 = np.diag(np.diag(dFdnu2))\n",
    "            dFdnu2[np.diag_indices(M)] += 1/sq_sigma_diag*norm.pdf(-est_mean/sq_sigma_diag)/est_pri_beta\n",
    "            \n",
    "            \n",
    "            theta1 += -step * dFdnu1\n",
    "            theta2 += -step * dFdnu2\n",
    "            est_sigma = -np.linalg.inv(theta2)/2\n",
    "            est_mean = est_sigma @ theta1\n",
    "            \n",
    "            # update pri_beta by extreme value\n",
    "            sq_sigma_diag = np.sqrt(np.diag(est_sigma))\n",
    "            est_pri_beta = ((est_mean + 2*sq_sigma_diag*norm.pdf(-est_mean/sq_sigma_diag)-2*est_mean*norm.cdf(-est_mean/sq_sigma_diag))).mean() if self.pri_opt_flag else pri_beta\n",
    "            current_F = self._obj_func(train_X, train_Y, est_pri_beta, est_mean, est_sigma)\n",
    "            \n",
    "#             print(np.allclose(np.diag(np.diag(theta2)), theta2))\n",
    "#             print(np.allclose(np.diag(np.diag(est_sigma)), est_sigma))\n",
    "            \n",
    "            if is_trace and ite % trace_step == 0:\n",
    "                print(current_F, (dFdnu1**2).sum(), (dFdnu2**2).sum())            \n",
    "            \n",
    "            if ite > 0 and np.abs(current_F - F[ite-1]) < tol:\n",
    "                if is_trace:\n",
    "                    print(current_F, (dFdnu1**2).sum(), (dFdnu2**2).sum())                            \n",
    "                break\n",
    "            else:\n",
    "                F.append(current_F)\n",
    "            pass\n",
    "        \n",
    "        \n",
    "        self.F_ = F\n",
    "        self.mean_ = est_mean\n",
    "        self.sigma_ = est_sigma\n",
    "        self.pri_beta_ = est_pri_beta\n",
    "        \n",
    "        return self\n",
    "        pass\n",
    "    \n",
    "    def predict(self, test_X: np.ndarray):\n",
    "        if not hasattr(self, \"mean_\"):\n",
    "            raise ValueError(\"fit has not finished yet, should fit before predict.\")\n",
    "        return test_X @ self.mean_\n",
    "        pass\n",
    "        \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VBNormal(BaseEstimator, RegressorMixin):\n",
    "    def __init__(\n",
    "        self, pri_beta: float = 20, pri_opt_flag: bool = True,\n",
    "        seed: int = -1, iteration: int = 1000, tol: float = 1e-8, step: float = 0.1,\n",
    "        is_trace: bool = False, trace_step: int = 20\n",
    "    ):\n",
    "        self.pri_beta = pri_beta\n",
    "        self.pri_opt_flag = pri_opt_flag\n",
    "        self.seed = seed\n",
    "        self.iteration = iteration\n",
    "        self.tol = tol\n",
    "        self.step = step\n",
    "        self.is_trace = is_trace\n",
    "        self.trace_step = trace_step\n",
    "        pass\n",
    "    \n",
    "    def _initialization(self, M: int):\n",
    "        seed = self.seed\n",
    "        \n",
    "        if seed > 0:\n",
    "            np.random.seed(seed)\n",
    "        \n",
    "        mean = np.random.normal(size = M)\n",
    "        sigma = invwishart.rvs(df = M+2, scale = np.eye(M), size = 1)\n",
    "        pri_beta = np.random.gamma(shape = 3, size = 1) if self.pri_opt_flag else self.pri_beta\n",
    "        \n",
    "        self.mean_ = mean\n",
    "        self.sigma_ = sigma\n",
    "        self.pri_beta_ = pri_beta\n",
    "        pass\n",
    "    \n",
    "    def _obj_func(self, X:np.ndarray, y:np.ndarray, pri_beta:float, mean:np.ndarray, sigma:np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Calculate objective function.\n",
    "\n",
    "        + Input:\n",
    "            1. X: input matrix (n, M) matrix\n",
    "            2. y: output vector (n, ) matrix\n",
    "            3. mean: mean parameter of vb posterior\n",
    "            4. sigma: covariance matrix of vb posterior\n",
    "\n",
    "        + Output:\n",
    "            value of the objective function.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        n, M = X.shape\n",
    "\n",
    "        log_2pi = np.log(2*np.pi)\n",
    "\n",
    "        F = 0\n",
    "        # const values\n",
    "        F += -M/2*log_2pi -M/2 + M*log_2pi + n*M/2*log_2pi + M*np.log(2*pri_beta)\n",
    "\n",
    "        F += ((y - X@mean)**2).sum()/2 - np.linalg.slogdet(sigma)[1]/2 + np.trace(X.T @ X @ sigma)/2\n",
    "\n",
    "        # term obtained from Normal prior\n",
    "        F += pri_beta/2*(mean@mean + np.trace(sigma)) - M/2*np.log(pri_beta) + M/2*log_2pi\n",
    "        \n",
    "        return F\n",
    "    \n",
    "    def fit(self, train_X:np.ndarray, train_Y:np.ndarray):\n",
    "        pri_beta = self.pri_beta\n",
    "        iteration = self.iteration\n",
    "        step = self.step\n",
    "        tol = self.tol\n",
    "        \n",
    "        is_trace = self.is_trace\n",
    "        trace_step = self.trace_step\n",
    "        \n",
    "        M = train_X.shape[1]\n",
    "        \n",
    "        if not hasattr(self, \"mean_\"):\n",
    "            self._initialization(M)\n",
    "        \n",
    "        est_mean = self.mean_\n",
    "        est_sigma = self.sigma_\n",
    "        est_pri_beta = self.pri_beta_\n",
    "                \n",
    "        F = []\n",
    "        XY_cov = train_Y@train_X\n",
    "        X_cov = train_X.T@train_X\n",
    "        \n",
    "        for ite in range(iteration):\n",
    "            sigma_inv = X_cov + est_pri_beta*np.eye(M)\n",
    "            est_mean = np.linalg.solve(sigma_inv, XY_cov)\n",
    "            est_sigma = np.linalg.inv(sigma_inv)\n",
    "            \n",
    "            # update pri_beta by extreme value\n",
    "            est_pri_beta = M/(est_mean@est_mean + np.trace(est_sigma)) if self.pri_opt_flag else pri_beta\n",
    "            current_F = self._obj_func(train_X, train_Y, est_pri_beta, est_mean, est_sigma)\n",
    "            if is_trace and ite % trace_step == 0:\n",
    "                print(current_F, (dFdnu1**2).sum(), (dFdnu2**2).sum())            \n",
    "            \n",
    "            if ite > 0 and np.abs(current_F - F[ite-1]) < tol:\n",
    "                if is_trace:\n",
    "                    print(current_F, (dFdnu1**2).sum(), (dFdnu2**2).sum())                            \n",
    "                break\n",
    "            else:\n",
    "                F.append(current_F)\n",
    "            pass\n",
    "        \n",
    "        \n",
    "        self.F_ = F\n",
    "        self.mean_ = est_mean\n",
    "        self.sigma_ = est_sigma\n",
    "        self.pri_beta_ = est_pri_beta\n",
    "        \n",
    "        return self\n",
    "        pass\n",
    "    \n",
    "    def predict(self, test_X: np.ndarray):\n",
    "        if not hasattr(self, \"mean_\"):\n",
    "            raise ValueError(\"fit has not finished yet, should fit before predict.\")\n",
    "        return test_X @ self.mean_\n",
    "        pass\n",
    "        \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VBApproxLaplace(BaseEstimator, RegressorMixin):\n",
    "    \"\"\"\n",
    "    Laplace prior is approximated by normal distribution, and approximated posterior distribution is obtained by the approximated laplace prior.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, pri_beta: float = 20, pri_opt_flag: bool = True,\n",
    "        seed: int = -1, iteration: int = 1000, tol: float = 1e-8, step: float = 0.1,\n",
    "        is_trace: bool = False, trace_step: int = 20\n",
    "    ):\n",
    "        self.pri_beta = pri_beta\n",
    "        self.pri_opt_flag = pri_opt_flag\n",
    "        self.seed = seed\n",
    "        self.iteration = iteration\n",
    "        self.tol = tol\n",
    "        self.step = step\n",
    "        self.is_trace = is_trace\n",
    "        self.trace_step = trace_step\n",
    "        pass\n",
    "    \n",
    "    def _initialization(self, M: int):\n",
    "        seed = self.seed\n",
    "        \n",
    "        if seed > 0:\n",
    "            np.random.seed(seed)\n",
    "        \n",
    "        mean = np.random.normal(size = M)\n",
    "        sigma = invwishart.rvs(df = M+2, scale = np.eye(M), size = 1)\n",
    "        pri_beta = np.random.gamma(shape = 3, size = 1) if self.pri_opt_flag else self.pri_beta\n",
    "        \n",
    "        self.mean_ = mean\n",
    "        self.sigma_ = sigma\n",
    "        self.pri_beta_ = pri_beta\n",
    "        pass\n",
    "    \n",
    "    def _obj_func(self, y:np.ndarray, pri_beta:float, mean:np.ndarray, inv_sigma:np.ndarray, h_xi: np.ndarray, v_xi: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Calculate objective function.\n",
    "\n",
    "        + Input:\n",
    "            1. X: input matrix (n, M) matrix\n",
    "            2. y: output vector (n, ) matrix\n",
    "            3. mean: mean parameter of vb posterior\n",
    "            4. sigma: covariance matrix of vb posterior\n",
    "\n",
    "        + Output:\n",
    "            value of the objective function.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        F = 0\n",
    "        F += pri_beta/2*np.sqrt(h_xi).sum() + v_xi@h_xi - M*np.log(pri_beta/2)\n",
    "        F += n/2*np.log(2*np.pi) + train_Y@train_Y/2 - mean @ (inv_sigma @ mean)/2 + np.linalg.slogdet(inv_sigma)[0]/2\n",
    "        return F\n",
    "    \n",
    "    def fit(self, train_X:np.ndarray, train_Y:np.ndarray):\n",
    "        iteration = self.iteration\n",
    "        step = self.step\n",
    "        tol = self.tol\n",
    "        \n",
    "        is_trace = self.is_trace\n",
    "        trace_step = self.trace_step\n",
    "        \n",
    "        M = train_X.shape[1]\n",
    "        \n",
    "        if not hasattr(self, \"mean_\"):\n",
    "            self._initialization(M)\n",
    "        \n",
    "        est_mean = self.mean_\n",
    "        est_sigma = self.sigma_\n",
    "        est_pri_beta = self.pri_beta_\n",
    "                \n",
    "        F = []\n",
    "        X_cov = train_X.T@train_X\n",
    "        XY_cov = train_X.T @ train_Y\n",
    "        \n",
    "        for ite in range(iteration):\n",
    "            # update form of approximated laplace prior\n",
    "            est_h_xi = est_mean**2 + np.diag(est_sigma)\n",
    "            est_v_xi = -est_pri_beta/2/np.sqrt(est_h_xi)            \n",
    "            \n",
    "            # update posterior distribution\n",
    "            inv_sigma = X_cov -2*np.diag(est_v_xi)\n",
    "            est_mean = np.linalg.solve(inv_sigma, XY_cov)\n",
    "            est_sigma = np.linalg.inv(inv_sigma)\n",
    "            \n",
    "            # update pri_beta by extreme value\n",
    "            est_pri_beta = M/((est_mean**2 + np.diag(est_sigma))/(2*np.sqrt(est_h_xi))).sum() if self.pri_opt_flag else pri_beta\n",
    "            \n",
    "            current_F = self._obj_func(train_Y, est_pri_beta, est_mean, inv_sigma, est_h_xi, est_v_xi)\n",
    "            if is_trace and ite % trace_step == 0:\n",
    "                print(current_F)            \n",
    "            \n",
    "            if ite > 0 and np.abs(current_F - F[ite-1]) < tol:\n",
    "                if is_trace:\n",
    "                    print(current_F, (dFdnu1**2).sum(), (dFdnu2**2).sum())                            \n",
    "                break\n",
    "            else:\n",
    "                F.append(current_F)\n",
    "            pass\n",
    "        \n",
    "        \n",
    "        self.F_ = F\n",
    "        self.mean_ = est_mean\n",
    "        self.sigma_ = est_sigma\n",
    "        self.pri_beta_ = est_pri_beta\n",
    "        \n",
    "        return self\n",
    "        pass\n",
    "    \n",
    "    def predict(self, test_X: np.ndarray):\n",
    "        if not hasattr(self, \"mean_\"):\n",
    "            raise ValueError(\"fit has not finished yet, should fit before predict.\")\n",
    "        return test_X @ self.mean_\n",
    "        pass\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment part\n",
    "+ By some datasets are used for train and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_func = lambda X, y, coef: 1 - ((y - X@coef)**2).sum() / ((y - y.mean())**2).sum()\n",
    "score_vb_laplace_exact = np.zeros(datasets)\n",
    "score_vb_laplace_approx = np.zeros(datasets)\n",
    "score_vb_normal = np.zeros(datasets)\n",
    "score_ard = np.zeros(datasets)\n",
    "score_lasso = np.zeros(datasets)\n",
    "score_ridge = np.zeros(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "sq_error = lambda X, y, coef: ((y - X@coef)**2).mean()\n",
    "sq_error_vb_laplace_exact = np.zeros(datasets)\n",
    "sq_error_vb_laplace_approx = np.zeros(datasets)\n",
    "sq_error_vb_normal = np.zeros(datasets)\n",
    "sq_error_ard = np.zeros(datasets)\n",
    "sq_error_lasso = np.zeros(datasets)\n",
    "sq_error_ridge = np.zeros(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pri_beta': 10,\n",
       " 'pri_opt_flag': True,\n",
       " 'variance': 'diag',\n",
       " 'iteration': 10000,\n",
       " 'step': 0.2,\n",
       " 'is_trace': True,\n",
       " 'trace_step': 1}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ln_vb_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "ln_vb_params[\"variance\"] = \"diag\"\n",
    "ln_vb_params[\"is_trace\"] = True\n",
    "ln_vb_params[\"trace_step\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "92849.20075302289 18875326.398489423 359540.5075198414\n",
      "True\n",
      "True\n",
      "109975.05719224799 12050071.483889073 228639.24678796716\n",
      "True\n",
      "True\n",
      "116892.51099156705 7713431.384784327 146326.77462441372\n",
      "True\n",
      "True\n",
      "120562.93032500097 4936977.43210936 93654.6969691077\n",
      "True\n",
      "True\n",
      "122793.43863572853 3159817.484014558 59943.23527331648\n",
      "True\n",
      "True\n",
      "124262.00511404609 2022354.556807278 38366.419704742824\n",
      "True\n",
      "True\n",
      "125280.48652512468 1294343.706577247 24556.265168057358\n",
      "True\n",
      "True\n",
      "126012.50778262592 828400.0917172353 15717.133529282619\n",
      "True\n",
      "True\n",
      "126552.23815685244 530187.5113582017 10059.68652097304\n",
      "True\n",
      "True\n",
      "126957.71681295871 339326.7140303096 6438.663168839806\n",
      "True\n",
      "True\n",
      "127266.6430873462 217173.10654467097 4121.043373610755\n",
      "True\n",
      "True\n",
      "127504.53280414063 138993.2224516469 2637.660796757177\n",
      "True\n",
      "True\n",
      "127689.2293920282 88957.15756121138 1688.2277532437847\n",
      "True\n",
      "True\n",
      "127833.54198466054 56933.507459608576 1080.5466080308165\n",
      "True\n",
      "True\n",
      "127946.86170817289 36438.02300742064 691.6022403025806\n",
      "True\n",
      "True\n",
      "128036.19189306181 23320.697500114402 442.6594411506052\n",
      "True\n",
      "True\n",
      "128106.82745096936 14925.474959982068 283.3241237242699\n",
      "True\n",
      "True\n",
      "128162.81629102872 9552.448451796401 181.34178467469232\n",
      "True\n",
      "True\n",
      "128207.2808245865 6113.6585752314195 116.06806574302936\n",
      "True\n",
      "True\n",
      "128242.647008036 3912.7996405330955 74.28962350321521\n",
      "True\n",
      "True\n",
      "128270.81067050956 2504.228762375133 47.549300485540456\n",
      "True\n",
      "True\n",
      "128293.26030012283 1602.7299705839578 30.43411554930221\n",
      "True\n",
      "True\n",
      "128311.16896481827 1025.7622051979383 19.47950100271011\n",
      "True\n",
      "True\n",
      "128325.46394557356 656.4973988600841 12.467964851054244\n",
      "True\n",
      "True\n",
      "128336.88001205352 420.1644575541693 7.980202626585437\n",
      "True\n",
      "True\n",
      "128346.00052157369 268.90916437747603 5.107788232773714\n",
      "True\n",
      "True\n",
      "128353.28934209787 172.1043653438748 3.2692826464400704\n",
      "True\n",
      "True\n",
      "128359.11578790247 110.14839237165913 2.092534765792232\n",
      "True\n",
      "True\n",
      "128363.77418728298 70.49599347960549 1.3393482886363945\n",
      "True\n",
      "True\n",
      "128367.49929569566 45.11808982306265 0.8572648331020498\n",
      "True\n",
      "True\n",
      "128370.47847355605 28.87599591379376 0.5487027413782744\n",
      "True\n",
      "True\n",
      "128372.86133165855 18.48090513107215 0.3512043573872209\n",
      "True\n",
      "True\n",
      "128374.76738612121 11.827950629985484 0.2247932718110821\n",
      "True\n",
      "True\n",
      "128376.29214343059 7.569998066848474 0.14388230004743502\n",
      "True\n",
      "True\n",
      "128377.51194383691 4.844868953760541 0.09209415941212679\n",
      "True\n",
      "True\n",
      "128378.48782048894 3.1007610591053343 0.058946423620817964\n",
      "True\n",
      "True\n",
      "128379.26857691072 1.9845158376040224 0.03772971217727285\n",
      "True\n",
      "True\n",
      "128379.8932427987 1.2701085464322137 0.024149613513244624\n",
      "True\n",
      "True\n",
      "128380.39303478468 0.8128812553097167 0.015457438991215683\n",
      "True\n",
      "True\n",
      "128380.79292263131 0.5202515482396066 0.009893855507017034\n",
      "True\n",
      "True\n",
      "128381.11288069548 0.3329658209768522 0.006332777863695748\n",
      "True\n",
      "True\n",
      "128381.36888818376 0.21310121763401446 0.004053438762137051\n",
      "True\n",
      "True\n",
      "128381.57372880376 0.1363867589247398 0.002594499857650696\n",
      "True\n",
      "True\n",
      "128381.73763015757 0.0872887930921198 0.0016606739057854734\n",
      "True\n",
      "True\n",
      "128381.86877506776 0.05586563897121559 0.001062957131095973\n",
      "True\n",
      "True\n",
      "128381.9737105332 0.035754528407541314 0.0006803741709363053\n",
      "True\n",
      "True\n",
      "128382.05767484111 0.022883230752383923 0.0004354923884174981\n",
      "True\n",
      "True\n",
      "128382.12485923248 0.014645480600537072 0.00027874944032969194\n",
      "True\n",
      "True\n",
      "128382.17861722835 0.00937324389974956 0.00017842188626705568\n",
      "True\n",
      "True\n",
      "128382.22163209286 0.005998963368089047 0.00011420442670485501\n",
      "True\n",
      "True\n",
      "128382.25605081157 0.0038393924293480563 7.310017915229116e-05\n",
      "True\n",
      "True\n",
      "128382.28359128238 0.0024572469265216708 4.679017167366395e-05\n",
      "True\n",
      "True\n",
      "128382.30562807788 0.001572660934925649 2.994963488760076e-05\n",
      "True\n",
      "True\n",
      "128382.32326106385 0.001006517660753556 1.9170309512991792e-05\n",
      "True\n",
      "True\n",
      "128382.33737030173 0.000644180690120999 1.2270645754290575e-05\n",
      "True\n",
      "True\n",
      "128382.34865997755 0.000412281651622212 7.854280655702576e-06\n",
      "True\n",
      "True\n",
      "128382.35769355076 0.0002638641047555921 5.027431004787157e-06\n",
      "True\n",
      "True\n",
      "128382.36492187802 0.0001688754904471609 3.2180036405215436e-06\n",
      "True\n",
      "True\n",
      "128382.37070571669 0.00010808189101644999 2.0598123325829535e-06\n",
      "True\n",
      "True\n",
      "128382.3753337303 6.917341996831269e-05 1.3184676869473662e-06\n",
      "True\n",
      "True\n",
      "128382.37903689622 4.4271635226127704e-05 8.439409165264891e-07\n",
      "True\n",
      "True\n",
      "128382.38200003357 2.83342604147676e-05 5.402009136853514e-07\n",
      "True\n",
      "True\n",
      "128382.38437102748 1.8134191635045224e-05 3.4577955168579443e-07\n",
      "True\n",
      "True\n",
      "128382.38626821022 1.1606052286169745e-05 2.213319056131666e-07\n",
      "True\n",
      "True\n",
      "128382.3877862666 7.4279820702302975e-06 1.416737749443738e-07\n",
      "True\n",
      "True\n",
      "128382.38900096016 4.753978057894824e-06 9.068503769647321e-08\n",
      "True\n",
      "True\n",
      "128382.38997291369 3.0425904735729017e-06 5.8047369191589197e-08\n",
      "True\n",
      "True\n",
      "128382.39075063566 1.9472864036384717e-06 3.715610484656479e-08\n",
      "True\n",
      "True\n",
      "128382.39137294062 1.2462815447282251e-06 2.378365273203668e-08\n",
      "True\n",
      "True\n",
      "128382.3918708865 7.976318703795389e-07 1.522396126782199e-08\n",
      "True\n",
      "True\n",
      "128382.39226932476 5.104918759267739e-07 9.744903176227863e-09\n",
      "True\n",
      "True\n",
      "128382.3925881407 3.2671958865743573e-07 6.237752399144753e-09\n",
      "True\n",
      "True\n",
      "128382.3928432457 2.09103602293036e-07 3.9928177156535365e-09\n",
      "True\n",
      "True\n",
      "128382.39304737144 1.3382826803285767e-07 2.5558277828888984e-09\n",
      "True\n",
      "True\n",
      "128382.3932107056 8.565134801461015e-08 1.6360043110377408e-09\n",
      "True\n",
      "True\n",
      "128382.39334139964 5.481766714688666e-08 1.0472203129891877e-09\n",
      "True\n",
      "True\n",
      "128382.39344597633 3.508382200512566e-08 6.703358270777771e-10\n",
      "True\n",
      "True\n",
      "128382.39352965484 2.2453975796855373e-08 4.2908918460227533e-10\n",
      "True\n",
      "True\n",
      "128382.39359661138 1.4370755596365282e-08 2.7466509419714937e-10\n",
      "True\n",
      "True\n",
      "128382.3936501876 9.197418736053397e-09 1.7581670708951845e-10\n",
      "True\n",
      "True\n",
      "128382.39369305738 5.886434513948911e-09 1.1254276588557742e-10\n",
      "True\n",
      "True\n",
      "128382.39372736022 3.767373479055165e-09 7.204034792220073e-11\n",
      "True\n",
      "True\n",
      "128382.39375480807 2.411154492415498e-09 4.611421256054596e-11\n",
      "True\n",
      "True\n",
      "128382.39377677091 1.5431615840215301e-09 2.9518519587990595e-11\n",
      "True\n",
      "True\n",
      "128382.39379434483 9.876379498880526e-10 1.8895358371084638e-11\n",
      "True\n",
      "True\n",
      "128382.39380840678 6.320975931088958e-10 1.2095295442233174e-11\n",
      "True\n",
      "True\n",
      "128382.3938196587 4.045484168675122e-10 7.742453712966793e-12\n",
      "True\n",
      "True\n",
      "128382.39382866207 2.589147999992335e-10 4.956116978622269e-12\n",
      "True\n",
      "True\n",
      "128382.39383586623 1.6570791181434386e-10 3.172526627163373e-12\n",
      "True\n",
      "True\n",
      "128382.39384163076 1.060546259414059e-10 2.030812377758967e-12\n",
      "True\n",
      "True\n",
      "128382.39384624331 6.787596232447179e-11 1.299975390145541e-12\n",
      "True\n",
      "True\n",
      "128382.3938499341 4.344125634859805e-11 8.321493353420388e-13\n",
      "True\n",
      "True\n",
      "128382.3938528874 2.7802814442682665e-11 5.326822409619518e-13\n",
      "True\n",
      "True\n",
      "128382.39385525048 1.77940639802167e-11 3.4098555102314335e-13\n",
      "True\n",
      "True\n",
      "128382.3938571413 1.1388369106430766e-11 2.1827527999879916e-13\n",
      "True\n",
      "True\n",
      "128382.39385865431 7.28866378290044e-12 1.3972494383990504e-13\n",
      "True\n",
      "True\n",
      "128382.393859865 4.6648135757415454e-12 8.944254824294603e-14\n",
      "True\n",
      "True\n",
      "128382.3938608337 2.985524440602529e-12 5.725523674503144e-14\n",
      "True\n",
      "True\n",
      "128382.39386160886 1.9107638456173867e-12 3.665110712845964e-14\n",
      "True\n",
      "True\n",
      "128382.39386222906 1.2229068957109489e-12 2.3461718267247948e-14\n",
      "True\n",
      "True\n",
      "128382.3938627254 7.826720128473519e-13 1.5018734784890672e-14\n",
      "True\n",
      "True\n",
      "128382.39386312247 5.009174721812019e-13 9.614079986832724e-15\n",
      "True\n",
      "True\n",
      "128382.39386344021 3.2059189316621974e-13 6.154361306144304e-15\n",
      "True\n",
      "True\n",
      "128382.3938636945 2.0518182550953087e-13 3.939663158252139e-15\n",
      "True\n",
      "True\n",
      "128382.39386389796 1.313182983724684e-13 2.521947244432346e-15\n",
      "True\n",
      "True\n",
      "128382.39386406071 8.40449804228019e-14 1.614409484442129e-15\n",
      "True\n",
      "True\n",
      "128382.39386419096 5.378958400058418e-14 1.0334566081043777e-15\n",
      "True\n",
      "True\n",
      "128382.39386429521 3.4425844541730425e-14 6.615636873447517e-16\n",
      "True\n",
      "True\n",
      "128382.39386437862 2.2032869796210618e-14 4.234986078363222e-16\n",
      "True\n",
      "True\n",
      "128382.39386444534 1.410123960837094e-14 2.7110231260247854e-16\n",
      "True\n",
      "True\n",
      "128382.39386449875 9.024931741013772e-15 1.7354630515865137e-16\n",
      "True\n",
      "True\n",
      "128382.3938645415 5.776051299881022e-15 1.1109602270007622e-16\n",
      "True\n",
      "True\n",
      "128382.39386457566 3.6967136188409584e-15 7.111848050265501e-17\n",
      "True\n",
      "True\n",
      "128382.39386460303 2.365935415272059e-15 4.5526729570200205e-17\n",
      "True\n",
      "True\n",
      "128382.39386462492 1.5142167935131737e-15 2.914421498322668e-17\n",
      "True\n",
      "True\n",
      "128382.39386464244 9.691108774950873e-16 1.865687024328384e-17\n",
      "True\n",
      "True\n",
      "128382.39386465646 6.20239832349819e-16 1.1943331254252778e-17\n",
      "True\n",
      "True\n",
      "128382.39386466768 3.9696029680373094e-16 7.645650649018681e-18\n",
      "True\n",
      "True\n",
      "128382.39386467665 2.540583811786488e-16 4.894453424190774e-18\n",
      "128382.39386467665 2.540583811786488e-16 4.894453424190774e-18\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VBLaplace(is_trace=True, iteration=10000, pri_beta=10, pri_opt_flag=True,\n",
       "          seed=-1, step=0.2, tol=1e-08, trace_step=1, variance='diag')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vb_laplace_exact_obj = VBLaplace(**ln_vb_params)\n",
    "\n",
    "# data generation\n",
    "train_X = np.random.normal(size = (n, M))\n",
    "train_Y = train_X @ true_w + np.random.normal(size = n)\n",
    "\n",
    "vb_laplace_exact_obj.fit(train_X, train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00993731, 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.00920756, 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.00794714, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.        , 0.        , 0.        , ..., 0.01296585, 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.01229836,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.00969664]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vb_laplace_exact_obj.sigma_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00993731, 0.00920756, 0.00794714, 0.01077332, 0.00865557,\n",
       "       0.01218945, 0.01088755, 0.00849105, 0.01023444, 0.00936811,\n",
       "       0.01237105, 0.00910487, 0.0105398 , 0.0106561 , 0.01101786,\n",
       "       0.01198317, 0.00998481, 0.01096596, 0.01129393, 0.00859911,\n",
       "       0.01088822, 0.01018645, 0.0124968 , 0.01088129, 0.00993674,\n",
       "       0.01033463, 0.00807404, 0.00981549, 0.00842826, 0.0096859 ,\n",
       "       0.01049741, 0.01145693, 0.01060514, 0.00943052, 0.00987806,\n",
       "       0.00992774, 0.00845357, 0.00942973, 0.01004097, 0.01081384,\n",
       "       0.01067888, 0.00950251, 0.01190983, 0.0109627 , 0.01155085,\n",
       "       0.01207892, 0.00979357, 0.01085858, 0.00920896, 0.01014675,\n",
       "       0.00818336, 0.00866962, 0.00897798, 0.00956785, 0.01058597,\n",
       "       0.00977907, 0.00981974, 0.00906789, 0.01131356, 0.00816479,\n",
       "       0.01190088, 0.00861629, 0.01254633, 0.01218215, 0.0093641 ,\n",
       "       0.00993394, 0.01050513, 0.01027787, 0.00870318, 0.01063569,\n",
       "       0.01004705, 0.00799553, 0.00936932, 0.00995082, 0.01174516,\n",
       "       0.01092303, 0.00942678, 0.00917055, 0.00901872, 0.00943458,\n",
       "       0.01059801, 0.00875863, 0.01182076, 0.00866692, 0.00947448,\n",
       "       0.00752508, 0.01005633, 0.0085478 , 0.00921817, 0.00926824,\n",
       "       0.01129701, 0.00924134, 0.01023674, 0.01087025, 0.01086246,\n",
       "       0.00970443, 0.01042144, 0.00843057, 0.00917571, 0.00661873,\n",
       "       0.00898283, 0.00990928, 0.0098095 , 0.01050825, 0.01198309,\n",
       "       0.00922167, 0.01222967, 0.0097172 , 0.00925465, 0.01042397,\n",
       "       0.01181404, 0.00909277, 0.01097402, 0.01165746, 0.01194473,\n",
       "       0.01164028, 0.00847416, 0.00874445, 0.00887646, 0.00990421,\n",
       "       0.01149205, 0.01028152, 0.01129081, 0.01145822, 0.00901592,\n",
       "       0.01099415, 0.00901894, 0.00895688, 0.01042162, 0.01035924,\n",
       "       0.01122418, 0.01052407, 0.01346916, 0.01266632, 0.00906885,\n",
       "       0.00958311, 0.01406945, 0.00969233, 0.00754481, 0.00934103,\n",
       "       0.00941063, 0.01073664, 0.01092322, 0.00875134, 0.00951451,\n",
       "       0.00781217, 0.00865688, 0.01296585, 0.01229836, 0.00969664])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.diag(vb_laplace_exact_obj.sigma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "ln_vb_params[\"variance\"] = \"full\"\n",
    "ln_vb_params[\"is_trace\"] = True\n",
    "ln_vb_params[\"trace_step\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34899.96078703485 44836549.35076478 1498066.7226087423\n",
      "27089.439054738556 28652851.666948825 959396.1150594726\n",
      "23041.523100337807 18333669.86063216 614571.6097237729\n",
      "20526.658972785768 11733044.300308771 393241.28656629304\n",
      "18836.793352662375 7508084.607331431 251625.7321074433\n",
      "17652.457858222213 4804530.690146969 161032.27971021255\n",
      "16802.456315779884 3074666.9545456795 103044.47072593545\n",
      "16183.15708264365 1967700.0594764235 65985.11014243097\n",
      "15728.18775647413 1259217.3434293328 42247.04476274988\n",
      "15393.169921812807 805820.5272205375 27028.656781456222\n",
      "15146.459957773823 515798.8262713634 17294.209168740246\n",
      "14965.24576437733 330170.82957615214 11066.77847862587\n",
      "14832.68507697147 211315.56884619815 7081.745684861961\n",
      "14736.161825728375 135251.33406055975 4531.925537909295\n",
      "14666.024158755012 86579.81252036244 2901.630594946606\n",
      "14614.966308772853 55424.14756824791 1858.318412855615\n",
      "14577.55141350264 35480.14412692711 1190.0991904774605\n",
      "14549.770656696206 22712.826710415102 762.0774836883377\n",
      "14528.700571385765 14539.551104359773 487.9240494543848\n",
      "14512.247633966032 9307.671607784756 312.3233077813795\n",
      "14498.95083232242 5959.111554020275 199.8705736747071\n",
      "14487.821265824201 3816.0298149571736 127.89672820614653\n",
      "14478.213087140142 2444.109106370948 81.8533901023711\n",
      "14469.733211295115 1565.3231298263067 52.39781476918113\n",
      "14462.189112453902 1002.062253550537 33.55469682687415\n",
      "14455.541188047537 641.1369165064427 21.519168891319126\n",
      "14449.817272096687 410.3446296209397 13.856485693785737\n",
      "14445.018319780616 263.17956514900226 8.982675071109313\n",
      "14441.091605754887 169.41739893419083 5.8671113469303116\n",
      "14437.957538621426 109.55968623461155 3.8601411924552655\n",
      "14435.522297212305 71.21532302869332 2.5604201472384838\n",
      "14433.673385786293 46.54398859493204 1.7155112646100705\n",
      "14432.289722027792 30.57868556715855 1.1626709067306122\n",
      "14431.261024183106 20.18278874918176 0.7972116472423257\n",
      "14430.49809537268 13.375971756203777 0.552778340138535\n",
      "14429.932805619514 8.898960955985599 0.3874575070251454\n",
      "14429.51411091914 5.94298803474425 0.2745000378209905\n",
      "14429.203939428493 3.9843272852945115 0.19657187404575704\n",
      "14428.973950608326 2.681906349107918 0.14228608064926832\n",
      "14428.80311476711 1.812714894288796 0.10408608946612044\n",
      "14428.675891659557 1.230467024280715 0.07691979788237568\n",
      "14428.580842222376 0.8389175408233198 0.05738873702375464\n",
      "14428.509568908565 0.5745516882055492 0.043192804821271094\n",
      "14428.455913595337 0.39532063943035034 0.032764414058008264\n",
      "14428.415357920554 0.27329344113029214 0.025026372388079032\n",
      "14428.384580273909 0.18985227533473237 0.01923134658072037\n",
      "14428.361131554762 0.13254251571889653 0.014855271209737191\n",
      "14428.343199304105 0.09300173807949494 0.011526412718085842\n",
      "14428.329436657637 0.06559395691273184 0.008977975236493527\n",
      "14428.318838382886 0.04650607402693459 0.007016246594590641\n",
      "14428.310650922052 0.03314815033696396 0.0054990179369539096\n",
      "14428.304306934895 0.02375399672123559 0.00432081995147442\n",
      "14428.29937749373 0.017114257079699775 0.0034027087602109735\n",
      "14428.295537020558 0.012397406798233944 0.0026851130590739855\n",
      "14428.292537451205 0.009029281510056294 0.0021227616893555188\n",
      "14428.290189108 0.00661170479871994 0.001681041394842979\n",
      "14428.288346472244 0.004867324178777567 0.001333350092513982\n",
      "14428.286897552676 0.003602075615278048 0.0010591521077027437\n",
      "14428.285755906045 0.0026795470726909236 0.0008425347939422284\n",
      "14428.284854623163 0.002003402080526883 0.0006711277202212094\n",
      "14428.284141778458 0.0015052823573292388 0.0005352870770520562\n",
      "14428.283576974112 0.0011364484372502694 0.0004274761226855206\n",
      "14428.283128706415 0.0008619798322859508 0.00034179188045232865\n",
      "14428.282772352173 0.0006567343372351397 0.0002736018256984967\n",
      "14428.282488624485 0.0005025209768350772 0.00021926386473726674\n",
      "14428.282262385079 0.0003861135148739808 0.00017590976146913089\n",
      "14428.28208172839 0.00029784844951540057 0.0001412771360678774\n",
      "14428.281937273416 0.00023063109077994592 0.00011357880544723747\n",
      "14428.28182161483 0.00017922774200074525 9.140093415424537e-05\n",
      "14428.2817288966 0.00013975932106869327 7.362348181814446e-05\n",
      "14428.281654479972 0.00010933742301190893 5.935795134362315e-05\n",
      "14428.281594684433 8.580154781151863e-05 4.789859253385421e-05\n",
      "14428.281546585155 6.752849643606673e-05 3.868409185826642e-05\n",
      "14428.281507854304 5.329347947238384e-05 3.1267449486139956e-05\n",
      "14428.281476636488 4.2168447170735255e-05 2.529225955317418e-05\n",
      "14428.281451450788 3.3447330326880175e-05 2.0474006252813643e-05\n",
      "14428.281431113575 2.6590824757874983e-05 1.658529494271148e-05\n",
      "14428.281414677624 2.1185431954219614e-05 1.344417469776617e-05\n",
      "14428.281401383932 1.69129446150681e-05 1.090489303555024e-05\n",
      "14428.281390623608 1.3527618022748208e-05 8.850566707257424e-06\n",
      "14428.28138190762 1.0839021224425506e-05 7.187363986830427e-06\n",
      "14428.281374842774 8.6991030554684e-06 5.839880887922811e-06\n",
      "14428.281369112608 6.9923990859952655e-06 4.74746171679364e-06\n",
      "14428.281364462156 5.628588296528818e-06 3.8612674547110025e-06\n",
      "14428.281360685814 4.536814867978918e-06 3.1419371801581268e-06\n",
      "14428.281357617625 3.6613408611164034e-06 2.557720338761658e-06\n",
      "14428.281355123523 2.9582062009694423e-06 2.0829833122167738e-06\n",
      "14428.28135309512 2.3926539131139107e-06 1.6970139329130506e-06\n",
      "14428.281351444715 1.93713875012992e-06 1.3830634242159255e-06\n",
      "14428.281350101295 1.569782159743046e-06 1.127577795665945e-06\n",
      "14428.281349007322 1.2731698883892546e-06 9.195805739114235e-07\n",
      "14428.281348116141 1.0334134600231442e-06 7.501765642062182e-07\n",
      "14428.281347389904 8.394155629425775e-07 6.121525260447908e-07\n",
      "14428.281346797887 6.822934743637152e-07 4.996555257400695e-07\n",
      "14428.281346315129 5.549253422193245e-07 4.079336192124343e-07\n",
      "14428.281345921354 4.51592243374716e-07 3.3312660238309826e-07\n",
      "14428.28134560007 3.676951452284987e-07 2.720970204076619e-07\n",
      "14428.281345337864 2.995305759965379e-07 2.2229357860237036e-07\n",
      "14428.28134512382 2.4411244668480264e-07 1.81640658155441e-07\n",
      "14428.281344949053 1.9903023829294705e-07 1.4844888007663827e-07\n",
      "14428.281344806326 1.6233591194791118e-07 1.2134265766141727e-07\n",
      "14428.28134468974 1.3245356198306617e-07 9.920147050395068e-08\n",
      "14428.281344594488 1.0810710998490477e-07 8.111223081171467e-08\n",
      "14428.281344516654 8.826234328777563e-08 6.633062338929577e-08\n",
      "14428.281344453042 7.208038291658703e-08 5.424971189500942e-08\n",
      "14428.281344401046 5.888027035270583e-08 4.4374429667729524e-08\n",
      "14428.281344358538 4.8108848266342643e-08 3.630084393612542e-08\n",
      "14428.281344323781 3.931647956776425e-08 2.9699289710498995e-08\n",
      "14428.28134429536 3.2137453569831854e-08 2.4300646055151573e-08\n",
      "14428.281344272114 2.6274152764283542e-08 1.9885165199036446e-08\n",
      "14428.281344253104 2.1484248893548726e-08 1.627337371002362e-08\n",
      "14428.281344237552 1.7570337075894932e-08 1.3318660814385696e-08\n",
      "14428.281344224828 1.4371536022889023e-08 1.0901237488379433e-08\n",
      "14428.281344214416 1.175667713712543e-08 8.923211533769651e-09\n",
      "14428.2813442059 9.618776018958834e-09 7.3045710426362956e-09\n",
      "14428.2813442059 9.618776018958834e-09 7.3045710426362956e-09\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VBLaplace(is_trace=True, iteration=10000, pri_beta=10, pri_opt_flag=True,\n",
       "          seed=-1, step=0.2, tol=1e-08, trace_step=1, variance='fule')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vb_laplace_exact_obj = VBLaplace(**ln_vb_params)\n",
    "\n",
    "# data generation\n",
    "train_X = np.random.normal(size = (n, M))\n",
    "train_Y = train_X @ true_w + np.random.normal(size = n)\n",
    "\n",
    "vb_laplace_exact_obj.fit(train_X, train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'variance'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-8f2a9d6697ec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdataset_ind\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mvb_laplace_exact_obj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVBLaplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mln_vb_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mvb_laplace_approx_obj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVBApproxLaplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mln_vb_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mvb_normal_obj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVBNormal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mln_vb_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mlasso_obj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLassoCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mln_lasso_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'variance'"
     ]
    }
   ],
   "source": [
    "for dataset_ind in range(datasets):\n",
    "    vb_laplace_exact_obj = VBLaplace(**ln_vb_params)\n",
    "    vb_laplace_approx_obj = VBApproxLaplace(**ln_vb_params)\n",
    "    vb_normal_obj = VBNormal(**ln_vb_params)\n",
    "    lasso_obj = LassoCV(**ln_lasso_params)\n",
    "    ridge_obj = RidgeCV(**ln_ridge_params)\n",
    "    ard_obj = ARDRegression(**ln_ard_params)\n",
    "    \n",
    "    # data generation\n",
    "    train_X = np.random.normal(size = (n, M))\n",
    "    train_Y = train_X @ true_w + np.random.normal(size = n)\n",
    "\n",
    "    lasso_obj.fit(train_X, train_Y)\n",
    "    ridge_obj.fit(train_X, train_Y)\n",
    "    ard_obj.fit(train_X, train_Y)\n",
    "    vb_laplace_exact_obj.fit(train_X, train_Y)\n",
    "    vb_normal_obj.fit(train_X, train_Y)\n",
    "    vb_laplace_approx_obj.fit(train_X, train_Y)\n",
    "\n",
    "    test_X = np.random.normal(size = (N, M))\n",
    "    test_Y = test_X @ true_w + np.random.normal(size = N)\n",
    "    \n",
    "    ### evaluation by square error\n",
    "    sq_error_lasso[dataset_ind] = sq_error(test_X, test_Y, lasso_obj.coef_)\n",
    "    sq_error_ridge[dataset_ind] = sq_error(test_X, test_Y, ridge_obj.coef_)\n",
    "    sq_error_ard[dataset_ind] = sq_error(test_X, test_Y, ard_obj.coef_)\n",
    "    sq_error_vb_laplace_exact[dataset_ind] = sq_error(test_X, test_Y, vb_laplace_exact_obj.mean_)\n",
    "    sq_error_vb_normal[dataset_ind] = sq_error(test_X, test_Y, vb_normal_obj.mean_)\n",
    "    sq_error_vb_laplace_approx[dataset_ind] = sq_error(test_X, test_Y, vb_laplace_approx_obj.mean_)\n",
    "\n",
    "    print(\n",
    "        \"sq_error:\"\n",
    "        , sq_error_lasso[dataset_ind]\n",
    "        , sq_error_ridge[dataset_ind]\n",
    "        , sq_error_ard[dataset_ind]\n",
    "        , sq_error_vb_laplace_exact[dataset_ind]\n",
    "        , sq_error_vb_normal[dataset_ind]\n",
    "        , sq_error_vb_laplace_approx[dataset_ind]\n",
    "    )    \n",
    "    \n",
    "    ### evaluation by R^2 score\n",
    "    score_lasso[dataset_ind] = score_func(test_X, test_Y, lasso_obj.coef_)\n",
    "    score_ridge[dataset_ind] = score_func(test_X, test_Y, ridge_obj.coef_)\n",
    "    score_ard[dataset_ind] = score_func(test_X, test_Y, ard_obj.coef_)\n",
    "    score_vb_laplace_exact[dataset_ind] = score_func(test_X, test_Y, vb_laplace_exact_obj.mean_)\n",
    "    score_vb_normal[dataset_ind] = score_func(test_X, test_Y, vb_normal_obj.mean_)\n",
    "    score_vb_laplace_approx[dataset_ind] = score_func(test_X, test_Y, vb_laplace_approx_obj.mean_)\n",
    "    \n",
    "    print(\n",
    "        \"R^2 score:\"\n",
    "        , score_lasso[dataset_ind]\n",
    "        , score_ridge[dataset_ind]\n",
    "        , score_ard[dataset_ind]\n",
    "        , score_vb_laplace_exact[dataset_ind]\n",
    "        , score_vb_normal[dataset_ind]\n",
    "        , score_vb_laplace_approx[dataset_ind]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "393.94768472453995 308.2709538942517 572.4249348002895 293.35773252079895 301.32563165797364 304.6139525695377\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    sq_error_lasso.mean()\n",
    "    , sq_error_ridge.mean()\n",
    "    , sq_error_ard.mean()\n",
    "    , sq_error_vb_laplace_exact.mean()\n",
    "    , sq_error_vb_normal.mean()\n",
    "    , sq_error_vb_laplace_approx.mean()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5560140895408846 0.6525618684686846 0.3547590596218103 0.6693602376627976 0.660415787359578 0.6566254504698098\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    score_lasso.mean()\n",
    "    , score_ridge.mean()\n",
    "    , score_ard.mean()\n",
    "    , score_vb_laplace_exact.mean()\n",
    "    , score_vb_normal.mean()\n",
    "    , score_vb_laplace_approx.mean()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  0.        ,  0.        ,  0.        , -0.76668197,\n",
       "        0.07758316,  0.        , -0.0995576 ,  0.        , -0.90644613,\n",
       "        0.        ,  0.        , -1.16645995,  1.00565264, -1.27819984,\n",
       "        0.30094245, -0.627204  ,  3.38047157,  1.22344481,  0.0473449 ,\n",
       "       -0.03814966,  0.        ,  0.        ,  3.52029545,  0.        ,\n",
       "        4.81596502,  0.        , -3.25347577,  0.66002526,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  2.63380714,  1.33677451,\n",
       "        0.        , -2.69622796,  3.58475763,  0.        ,  0.        ,\n",
       "        2.71306942,  0.        ,  1.88162727, -2.59235379,  3.48865255,\n",
       "        0.        ,  0.        ,  0.        ,  2.00792993, -0.4410573 ,\n",
       "        0.        , -4.15620415,  3.9016001 ,  0.        ,  6.5176337 ,\n",
       "       -3.96303101, -0.03691128,  1.35713552,  2.36783035,  0.        ,\n",
       "        0.        ,  0.18762913, -1.26910508, -3.83069906,  0.26426108,\n",
       "       -1.0842188 ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        2.82239462,  0.16265215,  0.        ,  0.        ,  0.        ,\n",
       "        1.42660064, -0.55960312, -1.35619478, -5.20937925,  0.        ,\n",
       "       -0.97513867,  0.        ,  5.61036017, -2.75649377, -4.37142083,\n",
       "        0.6528961 ,  3.65665585,  4.40974135, -0.18317878, -3.07988735,\n",
       "        0.        , -2.53854171,  3.27560309,  0.        ,  7.81317023,\n",
       "        0.        ,  0.        ,  0.        , -4.21542812,  0.        ,\n",
       "       -5.32304818,  0.        ,  0.        ,  1.46339651, -2.80157424,\n",
       "        2.40381311,  1.93661268,  0.        , -3.68530166,  3.68187137,\n",
       "        4.6779508 ,  1.30380009,  0.        ,  0.17820812,  0.        ,\n",
       "       -4.04242208,  0.        ,  0.        ,  0.        , -1.69246901,\n",
       "        1.35555417,  2.7963506 ,  0.        ,  3.07879094, -3.43986175,\n",
       "       -5.22595885,  0.39853712,  3.12602451, -4.18756991,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  4.31956641,\n",
       "       -1.51861904, -4.10136198,  0.30293722,  0.        ,  0.        ,\n",
       "        0.        , -0.21288762,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  2.77619219,  3.10719351,\n",
       "       -2.62922584,  1.34767249,  1.17694762,  0.        ,  0.82839278,\n",
       "        0.        , -2.43049864, -4.70204062,  0.31933329,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        , -3.37294018,\n",
       "        0.        ,  0.49227329,  1.88058534, -2.15501367,  0.        ,\n",
       "        2.51412901, -2.62329196,  1.88505711,  0.        , -1.44365597,\n",
       "        0.        , -5.9506191 ,  4.61010472, -0.3914984 , -2.56273853,\n",
       "        0.        ,  0.        , -0.03003618,  2.00666083, -1.11247555,\n",
       "        5.80470762, -0.75163024, -0.23380517,  0.67443324,  1.1577714 ,\n",
       "       -1.46410966,  0.        , -3.65760862, -1.67716493, -2.42723579,\n",
       "        0.        , -1.72682638, -0.84503449, -0.45390357,  0.        ,\n",
       "        0.        ,  0.        , -3.61775795, -1.11710953,  3.11547965,\n",
       "       -3.40428624,  1.72675508,  0.        , -1.09670842,  2.21402638,\n",
       "        0.        ,  5.68384374, 10.00605115,  0.        ,  0.        ,\n",
       "        0.        ,  0.        , -4.4211814 ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        , -1.12343038,  0.        ,  0.18522035,\n",
       "       -1.45337341,  0.        ,  2.88056761,  0.47835039,  1.22293896,\n",
       "        6.02282615, -0.55582293,  2.80719361, -9.04599777,  1.31709791,\n",
       "        0.        ,  1.41886036, -2.11526084,  1.31447898,  0.        ,\n",
       "       -1.88083653, -1.16022406,  0.        ,  0.        , -3.53824703,\n",
       "       -0.63372719,  0.        , -6.38107692,  5.22173563, -1.72092071,\n",
       "        1.34919695,  3.33451544,  2.68218189,  0.        , -9.42245829,\n",
       "        4.31844858,  0.        , -0.27071409, -1.75983284,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        , -3.71381013,\n",
       "       -4.36134198,  0.        ,  2.86892206,  4.43305466,  0.        ,\n",
       "        0.        ,  3.6775742 ,  0.        ,  0.        ,  0.        ,\n",
       "        1.08341558, -3.30639853,  2.06305632,  0.33057913,  0.19251094,\n",
       "        0.        ,  0.        , -0.88649413,  0.93654874, -0.92907885,\n",
       "        0.        ,  1.98561399,  0.        , -6.21216341, -1.44129124,\n",
       "        0.        , -2.44364738,  0.        ,  0.        , -4.60931771,\n",
       "        0.        , -7.1060601 , -0.25450493,  3.80237406, -1.03990865,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.50845406,\n",
       "        1.43603562,  0.        , -1.20824397,  0.34298881,  0.        ,\n",
       "       -0.10126385,  2.32899934,  0.        ,  0.        ,  2.53418494,\n",
       "        1.09601969, -1.3079937 , -1.43857833,  1.69509419,  0.        ,\n",
       "       -0.32535721,  1.34677404,  0.        ,  1.6209045 ,  0.        ,\n",
       "        0.37218781,  0.10660404, -2.18739269, -0.21103329,  3.88906184,\n",
       "        0.14417489, -0.44632924,  0.        , -3.08581264,  0.        ,\n",
       "       -0.64647768,  0.        ,  0.        ,  0.        ,  3.45132807,\n",
       "       -5.00554243, -0.69960904, -1.49639685,  0.        ,  0.        ,\n",
       "        0.        ,  0.82544241, -1.59146572,  2.98871443,  3.62286903,\n",
       "        1.42762964,  0.        ,  0.        ,  2.47423043,  0.        ,\n",
       "        0.        , -3.93509534, -3.67117534, -1.66381531,  1.56373128,\n",
       "       -2.54003703,  1.59270721,  0.        , -3.34179223,  0.        ,\n",
       "        1.65598468, -0.63192153,  3.2586072 ,  3.79248979,  1.8805576 ,\n",
       "        0.        ,  0.        ,  0.67639555,  2.12813618,  0.        ,\n",
       "       -2.03734815, -0.92737418, -0.76446219,  0.26149847, -1.01734604,\n",
       "        0.        ,  0.        ,  0.        ,  2.90633058, -1.7841167 ,\n",
       "        0.        ,  2.43839142,  1.72382498,  0.91379309, -4.73625657,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  1.76208384,\n",
       "       -5.52170122, -5.29286887,  3.40864884,  1.99232504,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  5.62730881,  0.        ,\n",
       "        0.        , -2.27439354,  0.4710198 ,  0.        ,  2.47498796,\n",
       "        0.        , -0.08208131, -4.96373925, -0.44566027,  0.        ,\n",
       "       -6.48977714,  0.        ,  0.        ,  2.27450145, -3.93815903,\n",
       "        3.3400167 ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.463294  ,  0.        , -0.97093426,\n",
       "        0.        ,  2.09674151,  2.34015509,  0.        ,  2.04765046,\n",
       "        0.        ,  2.79378918, -3.53025833,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        1.34513262,  1.83249431,  0.        ,  2.75072664, -0.09395582,\n",
       "        0.        , -4.52339131,  0.        ,  0.        ,  0.        ,\n",
       "        0.        , -0.13120524, -1.74165902,  0.        ,  2.79691941,\n",
       "        2.63991594,  0.        ,  3.2993635 , -2.01361589,  2.02896249,\n",
       "        1.14511249, -0.30690699,  2.8638261 ,  0.        , -1.90000553,\n",
       "        2.10753598,  1.25927173, -2.0796748 ,  0.        ,  0.8230681 ,\n",
       "        2.19885118,  2.00914115,  8.62446715,  0.        , -1.25440593,\n",
       "        2.36717472,  1.67806024,  0.45576057,  0.        ,  2.94290434,\n",
       "       -6.67368945,  4.84287936,  0.        , -1.54032161,  0.        ,\n",
       "       -1.45007199,  0.        ,  0.        ,  0.14619982,  1.24542242,\n",
       "       -1.1466085 ,  0.        ,  0.        , -0.87419234,  4.46233691,\n",
       "       -1.71233723, -5.01433667,  2.26989479,  3.50090393,  0.        ,\n",
       "        1.14726699,  0.        , -6.01328696,  1.50616649,  0.        ,\n",
       "        1.71264187,  0.        ,  1.61466992,  0.43339527,  0.        ,\n",
       "        0.        ,  0.04176741, -2.46692818, -1.69221917,  0.        ,\n",
       "        4.96165716,  5.70358511,  0.        ,  1.64405708,  3.47260936,\n",
       "       -4.14704621,  1.13520012,  4.72538745,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        , -0.72411832,  2.05994544,\n",
       "       -2.04054909, -5.93196356,  0.        , -6.38319172,  2.66130388,\n",
       "        0.54021669,  2.11793369, -5.40525212,  1.69551788,  0.        ,\n",
       "        0.        ,  1.63502311, -0.87789452, -1.26780838,  0.        ,\n",
       "       -1.69376272,  1.42150321,  0.75101169,  1.70050797,  0.        ,\n",
       "        6.76881923,  0.        , -1.72645458, -0.34222196,  0.        ,\n",
       "        0.        ,  0.        ,  4.20692684, -1.49838649, -0.42849111,\n",
       "        3.27441965,  0.        ,  0.        ,  1.33915726, -2.84127863,\n",
       "        0.        , -1.43255817,  0.        , -3.91065627,  0.        ,\n",
       "        3.28198124,  0.        ,  1.08150973, -0.74044771, -5.10018979,\n",
       "        0.98746237,  0.        ,  2.98754322, -3.93588581, -2.10669405,\n",
       "        0.        ,  0.        , -1.75450388,  0.80332304,  0.        ,\n",
       "        2.25687791,  0.        , -3.02195407, -0.34214784, -1.19599784,\n",
       "        0.        ,  0.        ,  1.20211888,  4.74596595, -3.36959436])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "+ We experimented the performance of the rigorously derived variational linear regression algorithm for the Laplace prior by comparing:\n",
    "    1. Ordinal optimized Lasso by cross-validation\n",
    "    2. Ordinal optimized Ridge by cross-validation\n",
    "    3. Variational Bayes linear regression for the normal prior\n",
    "    4. Bayesian ARD\n",
    "    5. Variational Bayes linear regression for the approximated Laplace prior.\n",
    "+ Results are as follows:\n",
    "    1. n > M with non-zero elements: ridge, vb for the normal prior gives the best performance, although vb for the Laplace prior gives better performance.\n",
    "    2. n > M with zero-elements: lasso, vb for the approximated Laplace gives the best performance. although vb for the Laplace prior also gives better performance.\n",
    "    3. M > n with zero-elements: results is similar with 1.\n",
    "    4. M > n with zero-elements: results is similar with 2.\n",
    "    5. M >> n, especially # of non-zero elements is larger than # of samples, vb for the Laplace prior gives the best performance.\n",
    "+ Summary of results:\n",
    "    + Derived algorithm can estimate every case, and # of features are extremely larger. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_json": true,
   "text_representation": {
    "extension": ".py",
    "format_name": "light",
    "format_version": "1.5",
    "jupytext_version": "1.5.2"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
