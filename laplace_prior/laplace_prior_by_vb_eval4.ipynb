{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression with laplace prior\n",
    " + In general, laplace prior gives sparse result for regression\n",
    "     + However, it is difficult to deal with it well due to non-differential point at the origin.\n",
    "         + $\\log p(w) \\equiv -1/\\beta \\sum_j |w_j| $, $|w_j|$ is non-differential at the origin.\n",
    " + By the way, non-differential point is eliminated by integrating $|w_j|$:\n",
    "     + $E[|w_j|]$ does not have non-diffenrential point when the distribution is normal distribution.\n",
    "     + It is achieved when we consider about the objective function of variational Bayes.\n",
    "         + $\\mathcal{F} := E[\\log \\frac{q(w)}{p(Y|X,w}p(w)]$\n",
    "         + Here, $\\mathcal{F}$ has a parameter that decides the form of $q(w) = N(w|m, \\Sigma)$, $(m, \\Sigma)$ is the parameter and optimized by it.\n",
    " + In this notebook, the approximated posterior distribution by Variational Bayes is studied.\n",
    "     + The objective function is optimized by a gradient descent method.\n",
    "         + Specifically, the Natural gradient descent is efficient method when we consider about a constrained parameter like positive definite matrix, positive real value, simplex, and so on.\n",
    "         + Thus, we used the natural gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Formulation\n",
    "+ Learning Model:\n",
    "    + $p(y|x,w) = N(y|x \\cdot w, 1), y \\in mathbb{R}, x,w \\in \\mathbb{R}^M$\n",
    "    + $p(w) \\equiv \\exp(-\\frac{1}{\\beta} \\sum_j |w_j|)$, $\\beta$ is hyperparameter.\n",
    "+ Approximated Variational Posterior distribution:\n",
    "    + $q(w) = N(w|m, \\Sigma)$\n",
    "        + $m \\in \\mathbb{R}^M, \\Sigma \\in \\mathbb{R}^{M \\times M}$ is the parameters to be optimized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In this notebook\n",
    "+ We compare the following average generalization error:\n",
    "$$\n",
    "    G(n) = \\frac{1}{L} \\sum_{j=1}^L \\| y - X \\hat{w}(x^l, y^l) \\|^2,\n",
    "$$\n",
    "where $\\hat{w}$ is estimated parameter by $(x^l, y^l)$.  \n",
    "We evaluate the error among Lasso, Ridge, and VB laplace(this calculation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminary\n",
    "## Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import invwishart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "from sklearn.linear_model import LassoCV, Lasso, LassoLarsCV\n",
    "from sklearn.linear_model import RidgeCV, Ridge\n",
    "from sklearn.linear_model import ARDRegression\n",
    "from sklearn.base import BaseEstimator, RegressorMixin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "## data setting\n",
    "n = 200 # train size\n",
    "M = 200 # # of features\n",
    "zero_ratio = 0.5\n",
    "n_zero_ind = int(M*zero_ratio) # # of zero elements in the parameter\n",
    "prob_seed = 20210112 # random seed\n",
    "\n",
    "N = 10000 # test size\n",
    "\n",
    "datasets = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(prob_seed)\n",
    "true_w = np.random.normal(scale = 3, size = M)\n",
    "zero_ind = np.random.choice(M, size = n_zero_ind)\n",
    "true_w[zero_ind] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "ln_vb_params = {\n",
    "    \"pri_beta\": 10,\n",
    "    \"pri_opt_flag\": True,\n",
    "    \"iteration\": 10000,\n",
    "    \"step\": 0.2,\n",
    "    \"is_trace\": False,\n",
    "    \"trace_step\": 100\n",
    "}\n",
    "ln_lasso_params = {\n",
    "    \"fit_intercept\": False,\n",
    "    \"cv\": 5,\n",
    "    \"max_iter\": 10000\n",
    "}\n",
    "ln_ridge_params = {\n",
    "    \"fit_intercept\": False,\n",
    "    \"cv\": 5\n",
    "}\n",
    "ln_ard_params = {\n",
    "    \"fit_intercept\": False\n",
    "}\n",
    "ln_vb_post_laplace_params = {\n",
    "    \"pri_beta\": 1,\n",
    "    \"iteration\": 1000,\n",
    "    \"is_trace\": False\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VBLaplace(BaseEstimator, RegressorMixin):\n",
    "    def __init__(\n",
    "        self, pri_beta: float = 20, pri_opt_flag: bool = True,\n",
    "        seed: int = -1, iteration: int = 1000, tol: float = 1e-8, step: float = 0.1,\n",
    "        is_trace: bool = False, trace_step: int = 20\n",
    "    ):\n",
    "        self.pri_beta = pri_beta\n",
    "        self.pri_opt_flag = pri_opt_flag\n",
    "        self.seed = seed\n",
    "        self.iteration = iteration\n",
    "        self.tol = tol\n",
    "        self.step = step\n",
    "        self.is_trace = is_trace\n",
    "        self.trace_step = trace_step\n",
    "        pass\n",
    "    \n",
    "    def _initialization(self, M: int):\n",
    "        seed = self.seed\n",
    "        \n",
    "        if seed > 0:\n",
    "            np.random.seed(seed)\n",
    "        \n",
    "        mean = np.random.normal(size = M)\n",
    "        sigma = invwishart.rvs(df = M+2, scale = np.eye(M), size = 1)\n",
    "        pri_beta = np.random.gamma(shape = 3, size = 1) if self.pri_opt_flag else self.pri_beta\n",
    "        \n",
    "        self.mean_ = mean\n",
    "        self.sigma_ = sigma\n",
    "        self.pri_beta_ = pri_beta\n",
    "        pass\n",
    "    \n",
    "    def _obj_func(self, X:np.ndarray, y:np.ndarray, pri_beta:float, mean:np.ndarray, sigma:np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Calculate objective function.\n",
    "\n",
    "        + Input:\n",
    "            1. X: input matrix (n, M) matrix\n",
    "            2. y: output vector (n, ) matrix\n",
    "            3. mean: mean parameter of vb posterior\n",
    "            4. sigma: covariance matrix of vb posterior\n",
    "\n",
    "        + Output:\n",
    "            value of the objective function.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        n, M = X.shape\n",
    "\n",
    "        sq_sigma_diag = np.sqrt(np.diag(sigma))\n",
    "        log_2pi = np.log(2*np.pi)\n",
    "\n",
    "        F = 0\n",
    "        # const values\n",
    "        F += -M/2*log_2pi -M/2 + M*log_2pi + n*M/2*log_2pi + M*np.log(2*pri_beta)\n",
    "\n",
    "        F += ((y - X@mean)**2).sum()/2 - np.linalg.slogdet(sigma)[1]/2 + np.trace(X.T @ X @ sigma)/2\n",
    "\n",
    "        # term obtained from laplace prior\n",
    "        F += ((mean + 2*sq_sigma_diag*norm.pdf(-mean/sq_sigma_diag)-2*mean*norm.cdf(-mean/sq_sigma_diag))/pri_beta).sum()\n",
    "\n",
    "        return F\n",
    "    \n",
    "    def fit(self, train_X:np.ndarray, train_Y:np.ndarray):\n",
    "        pri_beta = self.pri_beta\n",
    "        iteration = self.iteration\n",
    "        step = self.step\n",
    "        tol = self.tol\n",
    "        \n",
    "        is_trace = self.is_trace\n",
    "        trace_step = self.trace_step\n",
    "        \n",
    "        M = train_X.shape[1]\n",
    "        \n",
    "        if not hasattr(self, \"mean_\"):\n",
    "            self._initialization(M)\n",
    "        \n",
    "        est_mean = self.mean_\n",
    "        est_sigma = self.sigma_\n",
    "        est_pri_beta = self.pri_beta_\n",
    "        \n",
    "        # transformation to natural parameter\n",
    "        theta1 = np.linalg.solve(est_sigma, est_mean)\n",
    "        theta2 = -np.linalg.inv(est_sigma)/2        \n",
    "        \n",
    "        F = []\n",
    "        \n",
    "        cov_X = train_X.T @ train_X\n",
    "        cov_YX = train_Y @ train_X\n",
    "        for ite in range(iteration):\n",
    "            sq_sigma_diag = np.sqrt(np.diag(est_sigma))\n",
    "\n",
    "            # update mean and sigma by natural gradient\n",
    "            dFdnu1 = theta1 - cov_YX\n",
    "            dFdnu1 += (1 - 2*est_mean/sq_sigma_diag*norm.pdf(-est_mean/sq_sigma_diag) - 2*norm.cdf(-est_mean/sq_sigma_diag)) / est_pri_beta\n",
    "            dFdnu2 = theta2 + cov_X/2\n",
    "            dFdnu2[np.diag_indices(M)] += 1/sq_sigma_diag*norm.pdf(-est_mean/sq_sigma_diag)/est_pri_beta\n",
    "\n",
    "            theta1 += -step * dFdnu1\n",
    "            theta2 += -step * dFdnu2\n",
    "            est_sigma = -np.linalg.inv(theta2)/2\n",
    "            est_mean = est_sigma @ theta1\n",
    "            \n",
    "            # update pri_beta by extreme value\n",
    "            sq_sigma_diag = np.sqrt(np.diag(est_sigma))\n",
    "            est_pri_beta = ((est_mean + 2*sq_sigma_diag*norm.pdf(-est_mean/sq_sigma_diag)-2*est_mean*norm.cdf(-est_mean/sq_sigma_diag))).mean() if self.pri_opt_flag else pri_beta\n",
    "            current_F = self._obj_func(train_X, train_Y, est_pri_beta, est_mean, est_sigma)\n",
    "            if is_trace and ite % trace_step == 0:\n",
    "                print(current_F, (dFdnu1**2).sum(), (dFdnu2**2).sum())            \n",
    "            \n",
    "            if ite > 0 and np.abs(current_F - F[ite-1]) < tol:\n",
    "                if is_trace:\n",
    "                    print(current_F, (dFdnu1**2).sum(), (dFdnu2**2).sum())                            \n",
    "                break\n",
    "            else:\n",
    "                F.append(current_F)\n",
    "            pass\n",
    "        \n",
    "        \n",
    "        self.F_ = F\n",
    "        self.mean_ = est_mean\n",
    "        self.sigma_ = est_sigma\n",
    "        self.pri_beta_ = est_pri_beta\n",
    "        \n",
    "        return self\n",
    "        pass\n",
    "    \n",
    "    def predict(self, test_X: np.ndarray):\n",
    "        if not hasattr(self, \"mean_\"):\n",
    "            raise ValueError(\"fit has not finished yet, should fit before predict.\")\n",
    "        return test_X @ self.mean_\n",
    "        pass\n",
    "        \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VBNormal(BaseEstimator, RegressorMixin):\n",
    "    def __init__(\n",
    "        self, pri_beta: float = 20, pri_opt_flag: bool = True,\n",
    "        seed: int = -1, iteration: int = 1000, tol: float = 1e-8, step: float = 0.1,\n",
    "        is_trace: bool = False, trace_step: int = 20\n",
    "    ):\n",
    "        self.pri_beta = pri_beta\n",
    "        self.pri_opt_flag = pri_opt_flag\n",
    "        self.seed = seed\n",
    "        self.iteration = iteration\n",
    "        self.tol = tol\n",
    "        self.step = step\n",
    "        self.is_trace = is_trace\n",
    "        self.trace_step = trace_step\n",
    "        pass\n",
    "    \n",
    "    def _initialization(self, M: int):\n",
    "        seed = self.seed\n",
    "        \n",
    "        if seed > 0:\n",
    "            np.random.seed(seed)\n",
    "        \n",
    "        mean = np.random.normal(size = M)\n",
    "        sigma = invwishart.rvs(df = M+2, scale = np.eye(M), size = 1)\n",
    "        pri_beta = np.random.gamma(shape = 3, size = 1) if self.pri_opt_flag else self.pri_beta\n",
    "        \n",
    "        self.mean_ = mean\n",
    "        self.sigma_ = sigma\n",
    "        self.pri_beta_ = pri_beta\n",
    "        pass\n",
    "    \n",
    "    def _obj_func(self, X:np.ndarray, y:np.ndarray, pri_beta:float, mean:np.ndarray, sigma:np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Calculate objective function.\n",
    "\n",
    "        + Input:\n",
    "            1. X: input matrix (n, M) matrix\n",
    "            2. y: output vector (n, ) matrix\n",
    "            3. mean: mean parameter of vb posterior\n",
    "            4. sigma: covariance matrix of vb posterior\n",
    "\n",
    "        + Output:\n",
    "            value of the objective function.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        n, M = X.shape\n",
    "\n",
    "        log_2pi = np.log(2*np.pi)\n",
    "\n",
    "        F = 0\n",
    "        # const values\n",
    "        F += -M/2*log_2pi -M/2 + M*log_2pi + n*M/2*log_2pi + M*np.log(2*pri_beta)\n",
    "\n",
    "        F += ((y - X@mean)**2).sum()/2 - np.linalg.slogdet(sigma)[1]/2 + np.trace(X.T @ X @ sigma)/2\n",
    "\n",
    "        # term obtained from Normal prior\n",
    "        F += pri_beta/2*(mean@mean + np.trace(sigma)) - M/2*np.log(pri_beta) + M/2*log_2pi\n",
    "        \n",
    "        return F\n",
    "    \n",
    "    def fit(self, train_X:np.ndarray, train_Y:np.ndarray):\n",
    "        pri_beta = self.pri_beta\n",
    "        iteration = self.iteration\n",
    "        step = self.step\n",
    "        tol = self.tol\n",
    "        \n",
    "        is_trace = self.is_trace\n",
    "        trace_step = self.trace_step\n",
    "        \n",
    "        M = train_X.shape[1]\n",
    "        \n",
    "        if not hasattr(self, \"mean_\"):\n",
    "            self._initialization(M)\n",
    "        \n",
    "        est_mean = self.mean_\n",
    "        est_sigma = self.sigma_\n",
    "        est_pri_beta = self.pri_beta_\n",
    "                \n",
    "        F = []\n",
    "        XY_cov = train_Y@train_X\n",
    "        X_cov = train_X.T@train_X\n",
    "        \n",
    "        for ite in range(iteration):\n",
    "            sigma_inv = X_cov + est_pri_beta*np.eye(M)\n",
    "            est_mean = np.linalg.solve(sigma_inv, XY_cov)\n",
    "            est_sigma = np.linalg.inv(sigma_inv)\n",
    "            \n",
    "            # update pri_beta by extreme value\n",
    "            est_pri_beta = M/(est_mean@est_mean + np.trace(est_sigma)) if self.pri_opt_flag else pri_beta\n",
    "            current_F = self._obj_func(train_X, train_Y, est_pri_beta, est_mean, est_sigma)\n",
    "            if is_trace and ite % trace_step == 0:\n",
    "                print(current_F, (dFdnu1**2).sum(), (dFdnu2**2).sum())            \n",
    "            \n",
    "            if ite > 0 and np.abs(current_F - F[ite-1]) < tol:\n",
    "                if is_trace:\n",
    "                    print(current_F, (dFdnu1**2).sum(), (dFdnu2**2).sum())                            \n",
    "                break\n",
    "            else:\n",
    "                F.append(current_F)\n",
    "            pass\n",
    "        \n",
    "        \n",
    "        self.F_ = F\n",
    "        self.mean_ = est_mean\n",
    "        self.sigma_ = est_sigma\n",
    "        self.pri_beta_ = est_pri_beta\n",
    "        \n",
    "        return self\n",
    "        pass\n",
    "    \n",
    "    def predict(self, test_X: np.ndarray):\n",
    "        if not hasattr(self, \"mean_\"):\n",
    "            raise ValueError(\"fit has not finished yet, should fit before predict.\")\n",
    "        return test_X @ self.mean_\n",
    "        pass\n",
    "        \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VBApproxLaplace(BaseEstimator, RegressorMixin):\n",
    "    \"\"\"\n",
    "    Laplace prior is approximated by normal distribution, and approximated posterior distribution is obtained by the approximated laplace prior.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, pri_beta: float = 20, pri_opt_flag: bool = True,\n",
    "        seed: int = -1, iteration: int = 1000, tol: float = 1e-8, step: float = 0.1,\n",
    "        is_trace: bool = False, trace_step: int = 20\n",
    "    ):\n",
    "        self.pri_beta = pri_beta\n",
    "        self.pri_opt_flag = pri_opt_flag\n",
    "        self.seed = seed\n",
    "        self.iteration = iteration\n",
    "        self.tol = tol\n",
    "        self.step = step\n",
    "        self.is_trace = is_trace\n",
    "        self.trace_step = trace_step\n",
    "        pass\n",
    "    \n",
    "    def _initialization(self, M: int):\n",
    "        seed = self.seed\n",
    "        \n",
    "        if seed > 0:\n",
    "            np.random.seed(seed)\n",
    "        \n",
    "        mean = np.random.normal(size = M)\n",
    "        sigma = invwishart.rvs(df = M+2, scale = np.eye(M), size = 1)\n",
    "        pri_beta = np.random.gamma(shape = 3, size = 1) if self.pri_opt_flag else self.pri_beta\n",
    "        \n",
    "        self.mean_ = mean\n",
    "        self.sigma_ = sigma\n",
    "        self.pri_beta_ = pri_beta\n",
    "        pass\n",
    "    \n",
    "    def _obj_func(self, y:np.ndarray, pri_beta:float, mean:np.ndarray, inv_sigma:np.ndarray, h_xi: np.ndarray, v_xi: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Calculate objective function.\n",
    "\n",
    "        + Input:\n",
    "            1. X: input matrix (n, M) matrix\n",
    "            2. y: output vector (n, ) matrix\n",
    "            3. mean: mean parameter of vb posterior\n",
    "            4. sigma: covariance matrix of vb posterior\n",
    "\n",
    "        + Output:\n",
    "            value of the objective function.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        F = 0\n",
    "        F += pri_beta/2*np.sqrt(h_xi).sum() + v_xi@h_xi - M*np.log(pri_beta/2)\n",
    "        F += n/2*np.log(2*np.pi) + train_Y@train_Y/2 - mean @ (inv_sigma @ mean)/2 + np.linalg.slogdet(inv_sigma)[0]/2\n",
    "        return F\n",
    "    \n",
    "    def fit(self, train_X:np.ndarray, train_Y:np.ndarray):\n",
    "        iteration = self.iteration\n",
    "        step = self.step\n",
    "        tol = self.tol\n",
    "        \n",
    "        is_trace = self.is_trace\n",
    "        trace_step = self.trace_step\n",
    "        \n",
    "        M = train_X.shape[1]\n",
    "        \n",
    "        if not hasattr(self, \"mean_\"):\n",
    "            self._initialization(M)\n",
    "        \n",
    "        est_mean = self.mean_\n",
    "        est_sigma = self.sigma_\n",
    "        est_pri_beta = self.pri_beta_\n",
    "                \n",
    "        F = []\n",
    "        X_cov = train_X.T@train_X\n",
    "        XY_cov = train_X.T @ train_Y\n",
    "        \n",
    "        for ite in range(iteration):\n",
    "            # update form of approximated laplace prior\n",
    "            est_h_xi = est_mean**2 + np.diag(est_sigma)\n",
    "            est_v_xi = -est_pri_beta/2/np.sqrt(est_h_xi)            \n",
    "            \n",
    "            # update posterior distribution\n",
    "            inv_sigma = X_cov -2*np.diag(est_v_xi)\n",
    "            est_mean = np.linalg.solve(inv_sigma, XY_cov)\n",
    "            est_sigma = np.linalg.inv(inv_sigma)\n",
    "            \n",
    "            # update pri_beta by extreme value\n",
    "            est_pri_beta = M/((est_mean**2 + np.diag(est_sigma))/(2*np.sqrt(est_h_xi))).sum() if self.pri_opt_flag else pri_beta\n",
    "            \n",
    "            current_F = self._obj_func(train_Y, est_pri_beta, est_mean, inv_sigma, est_h_xi, est_v_xi)\n",
    "            if is_trace and ite % trace_step == 0:\n",
    "                print(current_F)            \n",
    "            \n",
    "            if ite > 0 and np.abs(current_F - F[ite-1]) < tol:\n",
    "                if is_trace:\n",
    "                    print(current_F, (dFdnu1**2).sum(), (dFdnu2**2).sum())                            \n",
    "                break\n",
    "            else:\n",
    "                F.append(current_F)\n",
    "            pass\n",
    "        \n",
    "        \n",
    "        self.F_ = F\n",
    "        self.mean_ = est_mean\n",
    "        self.sigma_ = est_sigma\n",
    "        self.pri_beta_ = est_pri_beta\n",
    "        \n",
    "        return self\n",
    "        pass\n",
    "    \n",
    "    def predict(self, test_X: np.ndarray):\n",
    "        if not hasattr(self, \"mean_\"):\n",
    "            raise ValueError(\"fit has not finished yet, should fit before predict.\")\n",
    "        return test_X @ self.mean_\n",
    "        pass\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LaplacePosteriorVB(BaseEstimator, RegressorMixin):\n",
    "\n",
    "    def __init__(self, pri_beta: float = 0.1, seed: int = -1, is_pri_optimize: bool = False, iteration: int = 1000, is_trace: bool = True):\n",
    "        \"\"\"\n",
    "        LaplacePosteriorVB is a class to calculate an approximated posterior distribution in the diagonal Laplace distribution q(w|lambda):\n",
    "        \n",
    "        q(w|lambda=(mu,sigma)) = prod_j=1^M sqrt{2}/sigma_j exp(-sqrt{2}/sigma_j |w_j-mu_j|),\n",
    "        where mu in mathbb{R}^M, sigma in mathbb{R}^M.\n",
    "        \n",
    "        The method searches an optimized posterior in terms of minimizing KL(q(w)||p(w|X^n,Y^n)),\n",
    "        where p(w|X^n,Y^n) propto p(Y|w,X) p(w), and p(Y|w,X)=N(Y|Xw, I_n), p(w)=q(w|0_M, pri_beta),\n",
    "        i.e. we consider here ordinal linear regression problem.        \n",
    "        \n",
    "        \"\"\"\n",
    "        self.pri_beta = pri_beta\n",
    "        self.seed = seed\n",
    "        self.iteration = iteration\n",
    "        self.is_trace = is_trace\n",
    "        pass\n",
    "    \n",
    "    def _initialize(self) -> (np.ndarray, np.ndarray, float):\n",
    "        \"\"\"\n",
    "        Initialize parameters for an approximated posterior distribution.\n",
    "        \"\"\"\n",
    "        if self.seed > 0:\n",
    "            np.random.seed(self.seed)\n",
    "        \n",
    "        est_mu = np.random.normal(size = M)\n",
    "        est_ln_sigma = np.random.normal(size = M)\n",
    "        est_pri_beta = self.pri_beta\n",
    "        return est_mu, est_ln_sigma, est_pri_beta\n",
    "        pass\n",
    "    \n",
    "    def _calc_energy(self, post_mu: np.ndarray, post_ln_sigma: np.ndarray, X:np.ndarray, y:np.ndarray, pri_beta: float) -> float:\n",
    "        \"\"\"\n",
    "        Objective function over parameters for the posterior.\n",
    "        \"\"\"\n",
    "        \n",
    "        post_sigma = np.exp(post_ln_sigma)\n",
    "        n, M = X.shape\n",
    "        energy = 0\n",
    "        energy += ((y-X@post_mu)**2).sum()/2 + (X**2).sum(axis=0)@post_sigma**2/2 + n/2*np.log(2*np.pi) - M - M*np.log(pri_beta)\n",
    "        energy += (-np.log(post_sigma) + np.sqrt(2)/pri_beta*np.abs(post_mu) + post_sigma/pri_beta*np.exp(-np.sqrt(2)/post_sigma*np.abs(post_mu))).sum()    \n",
    "        return energy    \n",
    "    \n",
    "    def _calc_energy_wrapper(self, est_params: np.ndarray, X:np.ndarray, y:np.ndarray, pri_beta: float) -> np.ndarray:\n",
    "        post_mu = est_params[:M]\n",
    "        post_ln_sigma = est_params[M:]\n",
    "        return self._calc_energy(post_mu, post_ln_sigma, X, y, pri_beta)\n",
    "        pass    \n",
    "    \n",
    "    def fit(self, X: np.ndarray, y: np.ndarray):\n",
    "        \n",
    "        (est_mu, est_ln_sigma, est_pri_beta) = self._initialize()\n",
    "        res = minimize(\n",
    "            fun=self._calc_energy_wrapper, x0=np.hstack([est_mu, est_ln_sigma]), \n",
    "            args=(train_X, train_Y, est_pri_beta), method = \"L-BFGS-B\", options={\"disp\":self.is_trace, \"maxiter\": self.iteration}\n",
    "        )\n",
    "        \n",
    "        est_mu = res.x[:M]\n",
    "        est_ln_sigma = res.x[M:]\n",
    "        est_sigma = np.exp(est_ln_sigma)\n",
    "        \n",
    "        self.mu_ = est_mu\n",
    "        self.sigma_ = est_sigma\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        check_is_fitted(self, \"mu_\")\n",
    "        return X@self.mu_\n",
    "        pass\n",
    "\n",
    "    def get_params(self, deep=True) -> dict:\n",
    "        return {'pri_beta': self.pri_beta}\n",
    "\n",
    "    def set_params(self, **parameters):\n",
    "        for parameter, value in parameters.items():\n",
    "            setattr(self,parameter, value)\n",
    "        return self\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment part\n",
    "+ By some datasets are used for train and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_func = lambda X, y, coef: 1 - ((y - X@coef)**2).sum() / ((y - y.mean())**2).sum()\n",
    "score_vb_laplace_exact = np.zeros(datasets)\n",
    "score_vb_laplace_approx = np.zeros(datasets)\n",
    "score_vb_normal = np.zeros(datasets)\n",
    "score_vb_post_laplace = np.zeros(datasets)\n",
    "score_ard = np.zeros(datasets)\n",
    "score_lasso = np.zeros(datasets)\n",
    "score_ridge = np.zeros(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "sq_error = lambda X, y, coef: ((y - X@coef)**2).mean()\n",
    "sq_error_vb_laplace_exact = np.zeros(datasets)\n",
    "sq_error_vb_laplace_approx = np.zeros(datasets)\n",
    "sq_error_vb_normal = np.zeros(datasets)\n",
    "sq_error_vb_post_laplace = np.zeros(datasets)\n",
    "sq_error_ard = np.zeros(datasets)\n",
    "sq_error_lasso = np.zeros(datasets)\n",
    "sq_error_ridge = np.zeros(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sq_error: 6.91860340924687 39.5136282801248 10.98276343249611 11.597422358379454 29.717932896476523 8.589198600222701 26.995210589881363\n",
      "R^2 score: 0.9933731672043422 0.9621527362860267 0.9894804005091341 0.9888916629146735 0.9715353286366555 0.9917730241776397 0.9741432285851767\n",
      "sq_error: 15.967262011444365 32.02510051461738 36.83528526132329 17.929945139228593 35.91158739772435 16.677310199846858 62.5787111086354\n",
      "R^2 score: 0.984680687473672 0.9692744740382631 0.964659485983871 0.9827976497804766 0.9655457003042868 0.9839994529514919 0.9399607251211344\n",
      "sq_error: 7.455486419268591 44.431008100260875 41.953703198383714 12.528396137120303 41.73281838741906 9.337131582745762 37.331503296413764\n",
      "R^2 score: 0.9930495288023334 0.9585786326045711 0.9608881313280662 0.9883202447691462 0.9610940539775453 0.9912953413788038 0.9651972354537834\n",
      "sq_error: 5.089478047633017 39.45275459976178 63.03033005216905 12.209329571528814 31.330613626970585 7.548798613619198 14.694533419848048\n",
      "R^2 score: 0.9950746060403195 0.9618191969039348 0.9390017593140024 0.9881842975722582 0.9696794811438215 0.992694573638714 0.9857792163618619\n",
      "sq_error: 10.257319882230432 72.46015141174362 41.679401143807546 14.079625842645816 43.59961523664151 13.41338097906952 63.81490318843187\n",
      "R^2 score: 0.9902328164636177 0.9310022885082673 0.9603122096871226 0.986593155784588 0.9584837512116929 0.9872275647664112 0.9392344317078716\n",
      "sq_error: 3.7853435749823454 31.43927861778565 9.997343225145418 10.055502856319581 26.51272300277145 6.144122393980297 21.986368931271322\n",
      "R^2 score: 0.9964051309250976 0.9701427127547214 0.9905057125517522 0.990450479451936 0.9748213692822953 0.9941650433708284 0.9791199619863329\n",
      "sq_error: 7.379755787432357 25.499438582454797 21.637453380563286 15.001898778511427 25.445957439012414 11.319168699583818 38.04684901499344\n",
      "R^2 score: 0.9929454386103772 0.9756242130414141 0.9793160170085529 0.9856591709885378 0.975675337498709 0.9891796188423068 0.9636297135312977\n",
      "sq_error: 6.371226427045783 31.04060230505684 27.093432872417214 10.035641138131501 18.112551034138768 7.506752431547899 18.090714025018016\n",
      "R^2 score: 0.9938799618716814 0.9701831865798234 0.9739747371868059 0.9903600182616699 0.9826015439571428 0.9927892044605572 0.9826225200329357\n",
      "sq_error: 12.615577768921007 28.435203320788872 16.823403115953273 15.51793984139138 34.08113716457507 12.156463218725042 17.265594776473257\n",
      "R^2 score: 0.9879039333576768 0.9727357620351231 0.9838693868034676 0.9851210909312886 0.967322328478332 0.9883441415112274 0.9834454046528041\n",
      "sq_error: 8.324226777368764 73.71719325926222 96.49498779109278 17.364692728097907 73.56952071258792 14.054744327740558 97.13754570919517\n",
      "R^2 score: 0.9918860017912475 0.9281445364167523 0.9059419956915191 0.9830738530485037 0.9282884794893335 0.9863002086139935 0.9053156655907251\n",
      "sq_error: 14.462739390453013 75.90057285688401 179.86032122996122 15.148087000996064 45.914871798855984 16.10916796069626 54.9616558837455\n",
      "R^2 score: 0.9863139439378538 0.9281754675083174 0.8297986036218151 0.9856654011157282 0.9565508654646568 0.9847559324777605 0.947989915092443\n",
      "sq_error: 11.55790936643813 24.373629253963813 48.514413788242955 16.800849106489444 24.64227511585101 13.679797926501319 37.017437835447076\n",
      "R^2 score: 0.9889886700674511 0.9767790121154296 0.95377985760489 0.983993671624063 0.9765230706534863 0.9869671267005508 0.9647331356960325\n",
      "sq_error: 6.357229193392172 36.11323943024542 46.37916443846589 15.23058581413305 31.14750012583884 10.921446821355897 19.58525678746558\n",
      "R^2 score: 0.9939045328070661 0.9653737407475524 0.9555305201888585 0.9853965409559705 0.9701350133236827 0.9895282490572871 0.9812211756755178\n",
      "sq_error: 15.265323855589866 37.66426133889632 11.370589447413542 14.812304557526376 23.232641503500258 14.47090085063233 28.4395157675346\n",
      "R^2 score: 0.985476480545362 0.9641659988694098 0.9891819539098909 0.9859074857864556 0.977896327385471 0.9862322992935825 0.9729424763969861\n",
      "sq_error: 10.01257417928765 19.325391520290975 26.69437386870379 8.464270893014021 18.551861257585912 7.068002155267469 21.07094507480733\n",
      "R^2 score: 0.9903331006911053 0.9813417997623352 0.9742272247194367 0.9918279502372889 0.9820886246075804 0.9931760140872302 0.9796565098311305\n",
      "sq_error: 13.862487403494558 32.97834763954611 3.491980657120672 19.47336712467763 33.35558682064421 15.59179592504864 38.754717494095146\n",
      "R^2 score: 0.9867808181798531 0.9685520526810161 0.9966700689511697 0.9814303181543961 0.9681923197428017 0.9851317603229066 0.9630437422930513\n",
      "sq_error: 6.754242294813398 47.32899313660844 6.2497567335880255 12.094530297515949 29.419412752677566 8.765551009900316 48.07518913426852\n",
      "R^2 score: 0.9938166737046084 0.9566715857942013 0.9942785166027771 0.9889277843531808 0.9730673226501958 0.9919753749292154 0.9559884635234188\n",
      "sq_error: 4.83684054100114 25.281687858734966 14.665977117625749 9.869103703808523 20.875604390951388 7.035245529070907 16.996665497465596\n",
      "R^2 score: 0.9954383245339394 0.9761565728147984 0.9861683618808258 0.9906923439265681 0.9803119967296506 0.9933649855405338 0.9839702650216833\n",
      "sq_error: 6.4710697591540605 44.340259831190615 55.78951095980101 10.595073532098075 51.38761592642999 8.476877860421094 64.73794506744481\n",
      "R^2 score: 0.9939391913853247 0.958470880586373 0.947747503702095 0.9900766464701422 0.9518703217726947 0.9920605500675816 0.9393663938476169\n",
      "sq_error: 8.571937587187415 58.98515397346606 16.013319739324313 12.491822296649737 30.777408250819207 11.502506372623456 47.7907171303473\n",
      "R^2 score: 0.9918856314916236 0.9441634670113049 0.9848414694944169 0.9881749897937104 0.9708654863921106 0.9891114961433078 0.9547603460558496\n",
      "sq_error: 7.557337321355884 21.891281513417756 72.48072249919696 10.874217796765162 21.073632786857395 8.802597558524575 16.080049623031606\n",
      "R^2 score: 0.9927538323140057 0.9790100812015797 0.9305036354871918 0.9895735227555985 0.9797940636452578 0.991559845052682 0.9845820384861833\n",
      "sq_error: 8.432898249054734 74.21857623028964 123.208675033419 16.543264583813485 71.6895697177547 12.072336067690346 42.305681525863626\n",
      "R^2 score: 0.9920668525103573 0.9301797680563526 0.8840929224833067 0.9844371230355724 0.9325589004819639 0.9886430952039228 0.9602014394953539\n",
      "sq_error: 10.28718637473152 43.945125314503684 8.503073788662599 17.17086916358646 45.030893004969364 13.137321555177268 43.780151214143274\n",
      "R^2 score: 0.9903805813126139 0.9589074656304268 0.9920488825687268 0.9839437360523979 0.95789217790924 0.9877154557265061 0.959061730838243\n",
      "sq_error: 11.909452657813038 20.615366683864174 12.622809035476251 16.126739637406306 16.430562890381925 14.426850978196518 14.269342618360874\n",
      "R^2 score: 0.9884450638511102 0.979998304492883 0.9877529424218442 0.9843533156263271 0.984058536479917 0.9860026025819711 0.986155422286882\n",
      "sq_error: 10.60260608322891 24.32658984253937 50.0788124249017 15.351663385751852 19.259184605805686 13.286763412569119 22.056332617473952\n",
      "R^2 score: 0.9900158750580419 0.9770924515639728 0.9528424317313846 0.9855438442108421 0.9818642601757358 0.9874882925062015 0.9792302780197996\n",
      "sq_error: 10.647511975441661 50.89115883957201 10.513916179970314 15.264971847644187 55.21989752355582 11.357884030330606 43.82157967607977\n",
      "R^2 score: 0.9900129170970668 0.9522654472209636 0.9901382263982382 0.9856818626074343 0.9482052055230165 0.9893466070125639 0.9588965243549921\n",
      "sq_error: 13.686815849392753 42.50728443540065 64.21277320687838 16.243648859088946 42.09025107717961 14.30595146646843 58.376906831612324\n",
      "R^2 score: 0.987063555812237 0.9598231525343185 0.9393076544703335 0.9846468996745839 0.9602173222829603 0.9864783639426971 0.9448235728900455\n",
      "sq_error: 9.600711052394233 48.43637791088606 11.503557437967089 11.171403572379507 29.840414698954852 11.436142172442246 45.40987324538293\n",
      "R^2 score: 0.9908165712756587 0.9536688457987752 0.9889964296153261 0.9893141468482989 0.9714565598321702 0.9890609147646553 0.9565638074039996\n",
      "sq_error: 8.183133280353458 44.54611322042012 15.251064849107697 9.289603722110018 26.656878811639146 9.107313618796987 27.332582404387853\n",
      "R^2 score: 0.9925967534813063 0.959699317324798 0.986202425295748 0.9915957342915495 0.975883633016792 0.9917606513870567 0.97527232679712\n",
      "sq_error: 7.487963500799939 27.095698668781576 6.053941347450265 10.852858904591164 20.631901290270527 9.257734286717533 23.6087327664654\n",
      "R^2 score: 0.9929224433852171 0.974389386202949 0.994277868391776 0.9897419741267048 0.98049891009259 0.9912496717522914 0.9776852354127941\n",
      "sq_error: 7.591795391850901 23.656333850481097 7.380224818817831 10.611749548616194 22.28318386382882 8.74811865590246 23.58804733896446\n",
      "R^2 score: 0.992701716359604 0.9772582603427331 0.9929051072536228 0.9897985187786044 0.9785783220101157 0.9915900985241055 0.9773239067813076\n",
      "sq_error: 13.66202938527833 35.15378626467397 69.67196467178456 18.660415936944617 38.002349145141416 16.455610040241144 60.57897430490828\n",
      "R^2 score: 0.9872922205568102 0.9673015955502271 0.9351944037408675 0.9826429556431491 0.9646519958608833 0.9846937627568084 0.9436522772238383\n",
      "sq_error: 10.392708250299926 38.89881438361154 50.55558605265972 19.010228439621418 28.577289470090875 14.10980800258967 38.874985001009584\n",
      "R^2 score: 0.9904521295575133 0.9642633247122718 0.9535541483365654 0.9825351396525209 0.9737457984111126 0.9870371980495843 0.964285216970991\n",
      "sq_error: 10.760611729303342 25.422068129981454 12.687524203676396 11.748743008171333 29.95937821762153 10.26541133298871 43.91030295291477\n",
      "R^2 score: 0.9899354094948905 0.9762222899628938 0.9881331341706225 0.9890111928297716 0.9719784635731319 0.9903985791856342 0.9589299168770399\n",
      "sq_error: 11.342998099976906 29.383459446181135 5.175491264653888 16.97656664109522 30.03172432897212 13.923827947169665 37.26825498182504\n",
      "R^2 score: 0.9891223237721197 0.9718219331879058 0.9950368220288036 0.9837198601502682 0.9712002619645879 0.9866473668665844 0.964260594271703\n",
      "sq_error: 10.56363121062782 39.86825343897821 6.465204153334642 14.14171021158707 22.310539061981235 12.308204661533859 38.843176718734334\n",
      "R^2 score: 0.9901238970765572 0.9627265505118058 0.993955580200948 0.986778695433572 0.9791415304396526 0.9884928682555852 0.9636849106619944\n",
      "sq_error: 9.425118586369145 25.867603287618014 14.102362414931578 13.371246730671853 26.220429053488537 10.20790559533734 45.59269011989731\n",
      "R^2 score: 0.9908510060008647 0.9748902313449194 0.9863107898404126 0.9870204862698708 0.9745477421990154 0.9900911520444372 0.955743023868725\n",
      "sq_error: 4.081724172146426 28.710421598563116 4.342992572754477 7.193511506017119 26.51953404898983 6.044816005443221 25.85713351822119\n",
      "R^2 score: 0.9962756772695137 0.9738035028209305 0.9960372858931972 0.9934363623596384 0.9758025532115708 0.9944844764856218 0.9764069530312016\n",
      "sq_error: 10.539184535020988 55.706353864956824 14.320047105553185 15.050545651871179 50.48835388380386 13.051840131353385 38.18866168669046\n",
      "R^2 score: 0.9901688086479195 0.9480358444665937 0.9866419339373281 0.985960508257058 0.9529033136829647 0.9878249462848178 0.9643767466757833\n",
      "sq_error: 8.727223175136032 77.85563182270398 9.171402907268048 18.70557001306898 71.63288833232522 12.540619107968585 76.95108146239151\n",
      "R^2 score: 0.9918466844945268 0.927264203356577 0.9914317143001669 0.98252452000302 0.9330777353321582 0.9882840598699781 0.9281092699178664\n",
      "sq_error: 6.284320358504251 31.55805240487035 5.0131026045678615 12.368910485720196 26.26826989137888 8.781926679107832 36.47534584006365\n",
      "R^2 score: 0.9939583809520235 0.9696607238890528 0.995180504102686 0.9881087785265247 0.9747462142794202 0.9915572325286326 0.964933336997907\n",
      "sq_error: 7.4912892045740795 50.40386541155954 5.8275932787557565 11.391097924491557 22.501902483430996 9.758078604029992 45.031246864379526\n",
      "R^2 score: 0.992802876962175 0.9515753816142257 0.9944012432711851 0.9890562050056189 0.9783816968834268 0.9906250993109316 0.9567370294512557\n",
      "sq_error: 11.633746256177574 35.426259985533015 11.108279659123369 16.824833540099345 37.02482346563857 11.5731067522681 38.09493573087156\n",
      "R^2 score: 0.9889048473444167 0.9662138270939892 0.9894059870445052 0.9839540856039338 0.9646892703905118 0.9889626794939255 0.9636686998296687\n",
      "sq_error: 8.633301629785297 31.999427867075738 16.714675082889652 20.22191903084225 31.349845633740212 14.337830419546968 29.66783189524316\n",
      "R^2 score: 0.9917188995573781 0.9693060097240831 0.9839672110204447 0.9806030473834915 0.969929091825308 0.9862470907510155 0.9715424867131117\n",
      "sq_error: 7.1449675909939 40.14750456272336 41.327898720372495 11.18377615226224 28.802143313712957 9.457656023819084 28.49008103628594\n",
      "R^2 score: 0.993251043042369 0.9620776753988857 0.9609627047203546 0.9894360859004403 0.9727942187230093 0.9910665356263771 0.973088984913567\n",
      "sq_error: 5.752556596716341 60.93640424840191 5.595295322561784 11.626068352679075 31.564166037629917 8.340386607384175 33.44262486672862\n",
      "R^2 score: 0.9944173477477019 0.9408633798373276 0.9945699642394473 0.9887173128011458 0.9693680957919699 0.9919059504594308 0.9675451180886122\n",
      "sq_error: 13.771834998954311 57.98033123960787 15.257864761631467 16.04339008681043 62.50727277928479 9.576277357169605 39.461618791609254\n",
      "R^2 score: 0.9869632573349885 0.9451144558397416 0.9855565466381175 0.9848129499044952 0.940829146589609 0.9909348707994485 0.9626447042554565\n",
      "sq_error: 6.216996583405597 40.95236240949442 21.41883827484924 13.510697372682323 33.19864817350008 9.285023190691064 28.999374136490868\n",
      "R^2 score: 0.9939657463851531 0.9602513950923482 0.979210749024567 0.9868864373067501 0.9677771959399672 0.9909879016337383 0.9718530361302598\n",
      "sq_error: 10.06663296705572 38.88626624511679 6.2093935014530794 16.7442699198469 40.282340980519905 13.03443293655866 34.63465209392124\n",
      "R^2 score: 0.9901401676035868 0.9619125810036074 0.9939181671361154 0.9835997104940363 0.9605451860713605 0.9872333356587705 0.9660768535148031\n",
      "sq_error: 5.072095178971859 31.75760517966222 12.716597326458553 11.882266839200081 28.482586210399763 7.9192587348246395 16.523513478694905\n",
      "R^2 score: 0.9951916303147059 0.969893643428359 0.9879446069273032 0.9887355560875404 0.9729983765563673 0.9924925060972041 0.9843356328101062\n",
      "sq_error: 5.233459804294284 19.8095501496542 14.621716768215768 10.405466459831414 21.749605859404994 7.936416579098366 18.03107416809874\n",
      "R^2 score: 0.9950066141122778 0.9810991711299197 0.9860490236105813 0.9900718623398893 0.9792481113788701 0.99242764977145 0.982796063281583\n",
      "sq_error: 8.89683858071269 24.1174049237108 64.50660993905947 11.632687325348018 25.7571490271487 9.943490878905399 51.948099565968015\n",
      "R^2 score: 0.9915940280384761 0.977213228301913 0.9390524229970623 0.989009124689973 0.9756639540476342 0.9906051228456646 0.9509180407799419\n",
      "sq_error: 10.50763305931182 36.574209113376234 11.702465021543796 11.2119443075162 18.9656155031305 10.106816620304384 26.212797939348132\n",
      "R^2 score: 0.9899466355022523 0.9650069760375034 0.988803458807551 0.9892727732101362 0.9818543106234436 0.9903301237469678 0.9749204401502637\n",
      "sq_error: 12.599861071005208 31.104728433960794 4.467885720698099 12.151783097786517 21.33703337466579 9.634239233333684 23.90806293152048\n",
      "R^2 score: 0.9880694430147073 0.9705475534212523 0.9957694481792956 0.9884937191049923 0.9797963889332604 0.9908775311461383 0.9773619323574245\n"
     ]
    }
   ],
   "source": [
    "for dataset_ind in range(datasets):\n",
    "    vb_laplace_exact_obj = VBLaplace(**ln_vb_params)\n",
    "    vb_laplace_approx_obj = VBApproxLaplace(**ln_vb_params)\n",
    "    vb_normal_obj = VBNormal(**ln_vb_params)\n",
    "    vb_post_laplace_obj = LaplacePosteriorVB(**ln_vb_post_laplace_params)\n",
    "    lasso_obj = LassoCV(**ln_lasso_params)\n",
    "    ridge_obj = RidgeCV(**ln_ridge_params)\n",
    "    ard_obj = ARDRegression(**ln_ard_params)\n",
    "    \n",
    "    # data generation\n",
    "    train_X = np.random.normal(size = (n, M))\n",
    "    train_Y = train_X @ true_w + np.random.normal(size = n)\n",
    "\n",
    "    lasso_obj.fit(train_X, train_Y)\n",
    "    ridge_obj.fit(train_X, train_Y)\n",
    "    ard_obj.fit(train_X, train_Y)\n",
    "    vb_laplace_exact_obj.fit(train_X, train_Y)\n",
    "    vb_normal_obj.fit(train_X, train_Y)\n",
    "    vb_laplace_approx_obj.fit(train_X, train_Y)\n",
    "    vb_post_laplace_obj.fit(train_X, train_Y)\n",
    "\n",
    "    test_X = np.random.normal(size = (N, M))\n",
    "    test_Y = test_X @ true_w + np.random.normal(size = N)\n",
    "    \n",
    "    ### evaluation by square error\n",
    "    sq_error_lasso[dataset_ind] = sq_error(test_X, test_Y, lasso_obj.coef_)\n",
    "    sq_error_ridge[dataset_ind] = sq_error(test_X, test_Y, ridge_obj.coef_)\n",
    "    sq_error_ard[dataset_ind] = sq_error(test_X, test_Y, ard_obj.coef_)\n",
    "    sq_error_vb_laplace_exact[dataset_ind] = sq_error(test_X, test_Y, vb_laplace_exact_obj.mean_)\n",
    "    sq_error_vb_normal[dataset_ind] = sq_error(test_X, test_Y, vb_normal_obj.mean_)\n",
    "    sq_error_vb_laplace_approx[dataset_ind] = sq_error(test_X, test_Y, vb_laplace_approx_obj.mean_)\n",
    "    sq_error_vb_post_laplace[dataset_ind] = sq_error(test_X, test_Y, vb_post_laplace_obj.mu_)\n",
    "\n",
    "    print(\n",
    "        \"sq_error:\"\n",
    "        , sq_error_lasso[dataset_ind]\n",
    "        , sq_error_ridge[dataset_ind]\n",
    "        , sq_error_ard[dataset_ind]\n",
    "        , sq_error_vb_laplace_exact[dataset_ind]\n",
    "        , sq_error_vb_normal[dataset_ind]\n",
    "        , sq_error_vb_laplace_approx[dataset_ind]\n",
    "        , sq_error_vb_post_laplace[dataset_ind]\n",
    "    )    \n",
    "    \n",
    "    ### evaluation by R^2 score\n",
    "    score_lasso[dataset_ind] = score_func(test_X, test_Y, lasso_obj.coef_)\n",
    "    score_ridge[dataset_ind] = score_func(test_X, test_Y, ridge_obj.coef_)\n",
    "    score_ard[dataset_ind] = score_func(test_X, test_Y, ard_obj.coef_)\n",
    "    score_vb_laplace_exact[dataset_ind] = score_func(test_X, test_Y, vb_laplace_exact_obj.mean_)\n",
    "    score_vb_normal[dataset_ind] = score_func(test_X, test_Y, vb_normal_obj.mean_)\n",
    "    score_vb_laplace_approx[dataset_ind] = score_func(test_X, test_Y, vb_laplace_approx_obj.mean_)\n",
    "    score_vb_post_laplace[dataset_ind] = score_func(test_X, test_Y, vb_post_laplace_obj.mu_)\n",
    "    \n",
    "    print(\n",
    "        \"R^2 score:\"\n",
    "        , score_lasso[dataset_ind]\n",
    "        , score_ridge[dataset_ind]\n",
    "        , score_ard[dataset_ind]\n",
    "        , score_vb_laplace_exact[dataset_ind]\n",
    "        , score_vb_normal[dataset_ind]\n",
    "        , score_vb_laplace_approx[dataset_ind]\n",
    "        , score_vb_post_laplace[dataset_ind]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0915412262039082 0.6321785216486484 0.059109827105147464 0.2011948442306489 0.5131901689200005 0.13334205510874567\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    sq_error_lasso.mean()\n",
    "    , sq_error_ridge.mean()\n",
    "    , sq_error_ard.mean()\n",
    "    , sq_error_vb_laplace_exact.mean()\n",
    "    , sq_error_vb_normal.mean()\n",
    "    , sq_error_vb_laplace_approx.mean()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    score_lasso.mean()\n",
    "    , score_ridge.mean()\n",
    "    , score_ard.mean()\n",
    "    , score_vb_laplace_exact.mean()\n",
    "    , score_vb_normal.mean()\n",
    "    , score_vb_laplace_approx.mean()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.69235959, 2.06668868, 1.97766328, 1.93799803, 2.58935789,\n",
       "       2.36709869, 1.86025766, 1.76518729, 1.94225541, 1.91558219,\n",
       "       2.2487974 , 2.30402386, 1.63204993, 2.27710986, 2.14034582,\n",
       "       1.79000666, 1.68010867, 1.74026175, 2.18191676, 0.82550489,\n",
       "       1.01354484, 2.68898204, 0.99007447, 1.6481237 , 3.13091457,\n",
       "       2.34343882, 1.14669674, 2.79494113, 2.0831621 , 0.79725522,\n",
       "       1.71175572, 3.03455942, 2.83632555, 2.30730855, 1.86995109,\n",
       "       1.88693774, 1.66701845, 3.48537482, 1.91144266, 2.39657277,\n",
       "       2.37034213, 2.20160332, 2.02695533, 2.57738414, 2.45129294,\n",
       "       2.44921173, 2.09647354, 1.47238528, 1.31498669, 2.44249975,\n",
       "       2.17063437, 2.57500058, 2.08018579, 2.05498443, 2.65922252,\n",
       "       0.56642172, 1.08040094, 2.65971181, 2.75270282, 2.97357094,\n",
       "       2.4881117 , 2.61614098, 1.19828121, 2.52375467, 2.09843451,\n",
       "       3.11763387, 1.43031712, 1.12253726, 1.8220084 , 1.48906877,\n",
       "       2.53769041, 1.6678076 , 4.01022484, 1.32909257, 2.05212481,\n",
       "       2.59280718, 0.99128902, 2.42638409, 2.32306339, 2.48595512,\n",
       "       1.10611227, 1.85107676, 2.7463841 , 2.43231457, 3.91895775,\n",
       "       4.27554591, 1.76160766, 1.66133014, 1.04274629, 2.02117509,\n",
       "       2.15954803, 2.20976033, 1.48492718, 1.62327774, 3.35753202,\n",
       "       2.43246979, 3.77998291, 2.39238013, 2.4465792 , 0.55333996])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.abs(vb_laplace_exact_obj.mean_ - np.sqrt(np.diag(vb_laplace_exact_obj.sigma_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upper = vb_laplace_exact_obj.mean_ + 0.8 * np.sqrt(np.diag(vb_laplace_exact_obj.sigma_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower = vb_laplace_exact_obj.mean_ - 0.8 * np.sqrt(np.diag(vb_laplace_exact_obj.sigma_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(((lower < 0) & (0 < upper)))[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(np.abs(vb_laplace_exact_obj.mean_) < 1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(lasso_obj.coef_ < 0.001).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(true_w < 0.001).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  0.        ,  0.        ,  0.        , -0.76668197,\n",
       "        0.07758316,  0.        , -0.0995576 ,  0.        , -0.90644613,\n",
       "        0.        ,  0.        , -1.16645995,  1.00565264, -1.27819984,\n",
       "        0.30094245, -0.627204  ,  3.38047157,  1.22344481,  0.0473449 ,\n",
       "       -0.03814966,  0.        ,  0.        ,  3.52029545,  0.        ,\n",
       "        4.81596502,  0.        , -3.25347577,  0.66002526,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  2.63380714,  1.33677451,\n",
       "        0.        , -2.69622796,  3.58475763,  0.        ,  0.        ,\n",
       "        2.71306942,  0.        ,  1.88162727, -2.59235379,  3.48865255,\n",
       "        0.        ,  0.        ,  0.        ,  2.00792993, -0.4410573 ,\n",
       "        0.        , -4.15620415,  3.9016001 ,  0.        ,  6.5176337 ,\n",
       "       -3.96303101, -0.03691128,  1.35713552,  2.36783035,  0.        ,\n",
       "        0.        ,  0.18762913, -1.26910508, -3.83069906,  0.26426108,\n",
       "       -1.0842188 ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        2.82239462,  0.16265215,  0.        ,  0.        ,  0.        ,\n",
       "        1.42660064, -0.55960312, -1.35619478, -5.20937925,  0.        ,\n",
       "       -0.97513867,  0.        ,  5.61036017, -2.75649377, -4.37142083,\n",
       "        0.6528961 ,  3.65665585,  4.40974135, -0.18317878, -3.07988735,\n",
       "        0.        , -2.53854171,  3.27560309,  0.        ,  7.81317023,\n",
       "        0.        ,  0.        ,  0.        , -4.21542812,  0.        ,\n",
       "       -5.32304818,  0.        ,  0.        ,  1.46339651, -2.80157424,\n",
       "        2.40381311,  1.93661268,  0.        , -3.68530166,  3.68187137,\n",
       "        4.6779508 ,  1.30380009,  0.        ,  0.17820812,  0.        ,\n",
       "       -4.04242208,  0.        ,  0.        ,  0.        , -1.69246901,\n",
       "        1.35555417,  2.7963506 ,  0.        ,  3.07879094, -3.43986175,\n",
       "       -5.22595885,  0.39853712,  3.12602451, -4.18756991,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  4.31956641,\n",
       "       -1.51861904, -4.10136198,  0.30293722,  0.        ,  0.        ,\n",
       "        0.        , -0.21288762,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  2.77619219,  3.10719351,\n",
       "       -2.62922584,  1.34767249,  1.17694762,  0.        ,  0.82839278,\n",
       "        0.        , -2.43049864, -4.70204062,  0.31933329,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        , -3.37294018,\n",
       "        0.        ,  0.49227329,  1.88058534, -2.15501367,  0.        ,\n",
       "        2.51412901, -2.62329196,  1.88505711,  0.        , -1.44365597,\n",
       "        0.        , -5.9506191 ,  4.61010472, -0.3914984 , -2.56273853,\n",
       "        0.        ,  0.        , -0.03003618,  2.00666083, -1.11247555,\n",
       "        5.80470762, -0.75163024, -0.23380517,  0.67443324,  1.1577714 ,\n",
       "       -1.46410966,  0.        , -3.65760862, -1.67716493, -2.42723579,\n",
       "        0.        , -1.72682638, -0.84503449, -0.45390357,  0.        ,\n",
       "        0.        ,  0.        , -3.61775795, -1.11710953,  3.11547965,\n",
       "       -3.40428624,  1.72675508,  0.        , -1.09670842,  2.21402638,\n",
       "        0.        ,  5.68384374, 10.00605115,  0.        ,  0.        ,\n",
       "        0.        ,  0.        , -4.4211814 ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        , -1.12343038,  0.        ,  0.18522035,\n",
       "       -1.45337341,  0.        ,  2.88056761,  0.47835039,  1.22293896,\n",
       "        6.02282615, -0.55582293,  2.80719361, -9.04599777,  1.31709791,\n",
       "        0.        ,  1.41886036, -2.11526084,  1.31447898,  0.        ,\n",
       "       -1.88083653, -1.16022406,  0.        ,  0.        , -3.53824703,\n",
       "       -0.63372719,  0.        , -6.38107692,  5.22173563, -1.72092071,\n",
       "        1.34919695,  3.33451544,  2.68218189,  0.        , -9.42245829,\n",
       "        4.31844858,  0.        , -0.27071409, -1.75983284,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        , -3.71381013,\n",
       "       -4.36134198,  0.        ,  2.86892206,  4.43305466,  0.        ,\n",
       "        0.        ,  3.6775742 ,  0.        ,  0.        ,  0.        ,\n",
       "        1.08341558, -3.30639853,  2.06305632,  0.33057913,  0.19251094,\n",
       "        0.        ,  0.        , -0.88649413,  0.93654874, -0.92907885,\n",
       "        0.        ,  1.98561399,  0.        , -6.21216341, -1.44129124,\n",
       "        0.        , -2.44364738,  0.        ,  0.        , -4.60931771,\n",
       "        0.        , -7.1060601 , -0.25450493,  3.80237406, -1.03990865,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.50845406,\n",
       "        1.43603562,  0.        , -1.20824397,  0.34298881,  0.        ,\n",
       "       -0.10126385,  2.32899934,  0.        ,  0.        ,  2.53418494,\n",
       "        1.09601969, -1.3079937 , -1.43857833,  1.69509419,  0.        ,\n",
       "       -0.32535721,  1.34677404,  0.        ,  1.6209045 ,  0.        ,\n",
       "        0.37218781,  0.10660404, -2.18739269, -0.21103329,  3.88906184,\n",
       "        0.14417489, -0.44632924,  0.        , -3.08581264,  0.        ,\n",
       "       -0.64647768,  0.        ,  0.        ,  0.        ,  3.45132807,\n",
       "       -5.00554243, -0.69960904, -1.49639685,  0.        ,  0.        ,\n",
       "        0.        ,  0.82544241, -1.59146572,  2.98871443,  3.62286903,\n",
       "        1.42762964,  0.        ,  0.        ,  2.47423043,  0.        ,\n",
       "        0.        , -3.93509534, -3.67117534, -1.66381531,  1.56373128,\n",
       "       -2.54003703,  1.59270721,  0.        , -3.34179223,  0.        ,\n",
       "        1.65598468, -0.63192153,  3.2586072 ,  3.79248979,  1.8805576 ,\n",
       "        0.        ,  0.        ,  0.67639555,  2.12813618,  0.        ,\n",
       "       -2.03734815, -0.92737418, -0.76446219,  0.26149847, -1.01734604,\n",
       "        0.        ,  0.        ,  0.        ,  2.90633058, -1.7841167 ,\n",
       "        0.        ,  2.43839142,  1.72382498,  0.91379309, -4.73625657,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  1.76208384,\n",
       "       -5.52170122, -5.29286887,  3.40864884,  1.99232504,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  5.62730881,  0.        ,\n",
       "        0.        , -2.27439354,  0.4710198 ,  0.        ,  2.47498796,\n",
       "        0.        , -0.08208131, -4.96373925, -0.44566027,  0.        ,\n",
       "       -6.48977714,  0.        ,  0.        ,  2.27450145, -3.93815903,\n",
       "        3.3400167 ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.463294  ,  0.        , -0.97093426,\n",
       "        0.        ,  2.09674151,  2.34015509,  0.        ,  2.04765046,\n",
       "        0.        ,  2.79378918, -3.53025833,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        1.34513262,  1.83249431,  0.        ,  2.75072664, -0.09395582,\n",
       "        0.        , -4.52339131,  0.        ,  0.        ,  0.        ,\n",
       "        0.        , -0.13120524, -1.74165902,  0.        ,  2.79691941,\n",
       "        2.63991594,  0.        ,  3.2993635 , -2.01361589,  2.02896249,\n",
       "        1.14511249, -0.30690699,  2.8638261 ,  0.        , -1.90000553,\n",
       "        2.10753598,  1.25927173, -2.0796748 ,  0.        ,  0.8230681 ,\n",
       "        2.19885118,  2.00914115,  8.62446715,  0.        , -1.25440593,\n",
       "        2.36717472,  1.67806024,  0.45576057,  0.        ,  2.94290434,\n",
       "       -6.67368945,  4.84287936,  0.        , -1.54032161,  0.        ,\n",
       "       -1.45007199,  0.        ,  0.        ,  0.14619982,  1.24542242,\n",
       "       -1.1466085 ,  0.        ,  0.        , -0.87419234,  4.46233691,\n",
       "       -1.71233723, -5.01433667,  2.26989479,  3.50090393,  0.        ,\n",
       "        1.14726699,  0.        , -6.01328696,  1.50616649,  0.        ,\n",
       "        1.71264187,  0.        ,  1.61466992,  0.43339527,  0.        ,\n",
       "        0.        ,  0.04176741, -2.46692818, -1.69221917,  0.        ,\n",
       "        4.96165716,  5.70358511,  0.        ,  1.64405708,  3.47260936,\n",
       "       -4.14704621,  1.13520012,  4.72538745,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        , -0.72411832,  2.05994544,\n",
       "       -2.04054909, -5.93196356,  0.        , -6.38319172,  2.66130388,\n",
       "        0.54021669,  2.11793369, -5.40525212,  1.69551788,  0.        ,\n",
       "        0.        ,  1.63502311, -0.87789452, -1.26780838,  0.        ,\n",
       "       -1.69376272,  1.42150321,  0.75101169,  1.70050797,  0.        ,\n",
       "        6.76881923,  0.        , -1.72645458, -0.34222196,  0.        ,\n",
       "        0.        ,  0.        ,  4.20692684, -1.49838649, -0.42849111,\n",
       "        3.27441965,  0.        ,  0.        ,  1.33915726, -2.84127863,\n",
       "        0.        , -1.43255817,  0.        , -3.91065627,  0.        ,\n",
       "        3.28198124,  0.        ,  1.08150973, -0.74044771, -5.10018979,\n",
       "        0.98746237,  0.        ,  2.98754322, -3.93588581, -2.10669405,\n",
       "        0.        ,  0.        , -1.75450388,  0.80332304,  0.        ,\n",
       "        2.25687791,  0.        , -3.02195407, -0.34214784, -1.19599784,\n",
       "        0.        ,  0.        ,  1.20211888,  4.74596595, -3.36959436])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "+ We experimented the performance of the rigorously derived variational linear regression algorithm for the Laplace prior by comparing:\n",
    "    1. Ordinal optimized Lasso by cross-validation\n",
    "    2. Ordinal optimized Ridge by cross-validation\n",
    "    3. Variational Bayes linear regression for the normal prior\n",
    "    4. Bayesian ARD\n",
    "    5. Variational Bayes linear regression for the approximated Laplace prior.\n",
    "+ Results are as follows:\n",
    "    1. n > M with non-zero elements: ridge, vb for the normal prior gives the best performance, although vb for the Laplace prior gives better performance.\n",
    "    2. n > M with zero-elements: lasso, vb for the approximated Laplace gives the best performance. although vb for the Laplace prior also gives better performance.\n",
    "    3. M > n with zero-elements: results is similar with 1.\n",
    "    4. M > n with zero-elements: results is similar with 2.\n",
    "    5. M >> n, especially # of non-zero elements is larger than # of samples, vb for the Laplace prior gives the best performance.\n",
    "+ Summary of results:\n",
    "    + Derived algorithm can estimate every case, and # of features are extremely larger. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_json": true,
   "text_representation": {
    "extension": ".py",
    "format_name": "light",
    "format_version": "1.5",
    "jupytext_version": "1.8.0"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
