{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test for Alternative Direction Method of Multipliers\n",
    "+ Object function $L(w)$\n",
    "$$\n",
    "L(w) = \\frac{1}{2} \\|Y - Xw\\|_2^2 + \\lambda \\|Aw\\|_1,\n",
    "$$\n",
    "where $w \\in \\mathbb{R}^M$, $X \\in \\mathbb{R}^{n \\times M}$, $Y \\in \\mathbb{R}^n$ , and $A \\in PSD(M)$ is hyperparameter.\n",
    "Here we want to optimize the parameter $w$.  \n",
    "Moreover, parameter w has zero elements with a zero_ratio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminary Section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_thresholing(x:np.ndarray, gamma:float):\n",
    "    \"\"\"\n",
    "    soft thresholding function S_gamma(x):\n",
    "    x >= gamma:\n",
    "        S_gamma(x) = x - gamma\n",
    "    |x| < gamma:\n",
    "        S_gamma(x) = 0\n",
    "    x <= -gamma:\n",
    "        S_gamma(x) = x + gamma\n",
    "        \n",
    "    + Input:\n",
    "        1. x: M-dim array\n",
    "        2. gamma: positive real value\n",
    "    \"\"\"\n",
    "    \n",
    "    pos_dom_ind = np.where(x >= gamma)[0]\n",
    "    neg_dom_ind = np.where(x <= -gamma)[0]\n",
    "    \n",
    "    ret_val = np.zeros(len(x))\n",
    "    ret_val[pos_dom_ind] = x[pos_dom_ind] - gamma\n",
    "    ret_val[neg_dom_ind] = x[neg_dom_ind] + gamma\n",
    "    \n",
    "    return ret_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 200\n",
    "M = 80\n",
    "data_seed = 20200726\n",
    "\n",
    "zero_ratio = 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generation\n",
    "$$\n",
    " y_i \\sim x_i w^* \\forall i,\n",
    "$$\n",
    "$w^* \\sim N(0,I_M)$, and some elements are zero. $x_i \\sim N(0,I_M)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(data_seed)\n",
    "train_X = np.random.normal(size = (n, M))\n",
    "true_w = np.random.normal(size = M)\n",
    "### some elements are zero.\n",
    "true_w[np.random.choice(np.arange(M), size = int(zero_ratio * M), replace = False)] = 0\n",
    "train_Y = np.random.normal(train_X @ true_w, size = n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"iteration\": 2000,\n",
    "    \"lam\": 10,\n",
    "    \"gamma\": 1,\n",
    "    \"A\": -1*np.eye(M) + (np.triu(np.ones((M, M)), k=1) - np.triu(np.ones((M, M)), k=2)),\n",
    "    ##\"A\": np.eye(M) + (np.tril(np.ones((M, M)), k=-1) - np.tril(np.ones((M, M)), k=-2))*0.5 + (np.triu(np.ones((M, M)), k=1) - np.triu(np.ones((M, M)), k=2))*0.5,\n",
    "    \"seed\": 20190726,\n",
    "    \"eps\": 1e-5\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### unpack for functionize\n",
    "gamma = params[\"gamma\"]\n",
    "lam = params[\"lam\"]\n",
    "seed = params[\"seed\"]\n",
    "iteration = params[\"iteration\"]\n",
    "A = params[\"A\"]\n",
    "eps = params[\"eps\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### initialization\n",
    "est_w = np.random.normal(size = M)\n",
    "est_z = np.random.normal(size = M)\n",
    "est_v = np.random.normal(size = M)\n",
    "# est_v = np.zeros(M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### put fixed values\n",
    "inv_S = np.linalg.inv(train_X.T @ train_X + 1/gamma * A.T @ A)\n",
    "loss_func = lambda x,y,w,A,lam: ((x@w - y)**2).sum()/2 + lam*(np.abs(A@w)).sum()\n",
    "generalized_lagrangian = lambda x,y,w,z,v,A,lam,gamma: ((x@w - y)**2).sum()/2 + lam*np.abs(z).sum()+v@(A@w - z)/gamma+((A@w - z)**2).sum()/(2*gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func_trace = np.zeros(iteration)\n",
    "lagrangian_trace = np.zeros(iteration)\n",
    "for ite in range(iteration):\n",
    "    est_w = inv_S @ (train_X.T @ train_Y + 1/gamma * A.T @ (est_z - est_v))\n",
    "    est_z = soft_thresholing(A @ est_w + est_v, gamma * lam)\n",
    "    est_v = est_v + A @ est_w - est_z\n",
    "    \n",
    "    loss_func_trace[ite] = loss_func(train_X, train_Y, est_w, A, lam)\n",
    "    lagrangian_trace[ite] = generalized_lagrangian(train_X, train_Y, est_w, est_z, est_v, A, lam, gamma)\n",
    "    if ite > 0 and np.abs(loss_func_trace[ite] - loss_func_trace[ite-1]) < eps:\n",
    "        loss_func_trace = loss_func_trace[:(ite+1)]\n",
    "        lagrangian_trace = lagrangian_trace[:(ite+1)]\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdf786b0b5574d4987b4d1a666a81763",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x241acf7b208>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.plot(loss_func_trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".py",
    "format_name": "light",
    "format_version": "1.5",
    "jupytext_version": "1.5.2"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
